{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9136a833",
      "metadata": {},
      "source": [
        "# Fixed Basic Inference Example for Tunix\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "\n",
        "- Initialize a Tunix tokenizer safely (with a mock fallback for local setups)\n",
        "- Create a dummy Gemma model using `dummy_model_creator`\n",
        "- Configure the `Sampler` for inference\n",
        "- Run inference on multiple prompts\n",
        "\n",
        "It is designed to run locally without requiring access to remote checkpoints, \n",
        "making it a beginner-friendly and reproducible example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ef939231",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"\n",
        "Fixed Basic Inference Example for Tunix (Notebook version).\n",
        "\n",
        "This notebook demonstrates how to correctly initialize a Tunix model\n",
        "(using dummy weights) and use the Sampler class for text generation.\n",
        "\"\"\"\n",
        "\n",
        "from flax import nnx\n",
        "import jax\n",
        "from tunix import Tokenizer\n",
        "from tunix.generate import sampler\n",
        "from tunix.models import dummy_model_creator\n",
        "from tunix.models.gemma import model as gemma_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03eab79f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Tokenizer...\n"
          ]
        }
      ],
      "source": [
        "print(\"Initializing Tokenizer...\")\n",
        "\n",
        "try:\n",
        "  tokenizer = Tokenizer()\n",
        "  print(\"Tokenizer initialized successfully.\")\n",
        "except Exception as e:\n",
        "  print(\n",
        "      f\"Warning: Could not initialize real tokenizer ({e}). Using mock\"\n",
        "      \" tokenizer.\"\n",
        "  )\n",
        "\n",
        "  class MockTokenizer:\n",
        "\n",
        "    def encode(self, s):\n",
        "      # Return dummy token IDs\n",
        "      return [1, 2, 3]\n",
        "\n",
        "    def decode(self, t):\n",
        "      # Always return a dummy string\n",
        "      return \"dummy output\"\n",
        "\n",
        "    def pad_id(self):\n",
        "      return 0\n",
        "\n",
        "    def bos_id(self):\n",
        "      return 1\n",
        "\n",
        "    def eos_id(self):\n",
        "      return 2\n",
        "\n",
        "  tokenizer = MockTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95ecc783",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Initializing Dummy Model (Gemma 2B config)...\")\n",
        "\n",
        "config = gemma_model.ModelConfig.gemma_2b()\n",
        "\n",
        "model = dummy_model_creator.create_dummy_model(\n",
        "    gemma_model.Transformer,\n",
        "    config,\n",
        "    dtype=jax.numpy.float32,\n",
        ")\n",
        "\n",
        "print(\"Dummy model created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3672597",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Initializing Sampler...\")\n",
        "\n",
        "cache_config = sampler.CacheConfig(\n",
        "    cache_size=1024,  # Max sequence length\n",
        "    num_layers=config.num_layers,\n",
        "    num_kv_heads=config.num_kv_heads,\n",
        "    head_dim=config.head_dim,\n",
        ")\n",
        "\n",
        "inference_sampler = sampler.Sampler(\n",
        "    transformer=model,\n",
        "    tokenizer=tokenizer,\n",
        "    cache_config=cache_config,\n",
        ")\n",
        "\n",
        "print(\"Sampler initialized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f9b15e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"If I have 3 apples and eat 1, how many remain?\",\n",
        "    \"Write a short story about a robot.\",\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Starting Generation...\")\n",
        "\n",
        "output = inference_sampler(\n",
        "    input_strings=prompts,\n",
        "    max_generation_steps=50,\n",
        "    temperature=0.7,\n",
        "    echo=True,  # include prompt in output\n",
        ")\n",
        "\n",
        "for i, text in enumerate(output.text):\n",
        "  print(\"\\n\" + \"-\" * 50)\n",
        "  print(f\"Prompt {i+1}: {prompts[i]}\")\n",
        "  print(f\"Generated: {text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34e1861b",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "- This example uses **dummy model weights** via `dummy_model_creator` so that:\n",
        "  - It does not require downloading large real checkpoints.\n",
        "  - It is safe to run on local CPUs/GPUs with limited memory.\n",
        "- The tokenizer is wrapped in a **try/except**:\n",
        "  - If a real tokenizer cannot be initialized (e.g., missing HF model or remote paths),\n",
        "    a lightweight mock tokenizer is used instead.\n",
        "- The `Sampler` is configured explicitly via `CacheConfig`, showing:\n",
        "  - `cache_size`\n",
        "  - `num_layers`\n",
        "  - `num_kv_heads`\n",
        "  - `head_dim`\n",
        "- This notebook is intended as a **reference/example** for:\n",
        "  - New contributors exploring the Tunix generation stack.\n",
        "  - Testing changes to `Sampler`, `dummy_model_creator`, or Gemma configs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c66e17cf",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
