{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNiTzJRoa1hf"
      },
      "source": [
        "# DPO Demo\n",
        "\n",
        "This tutorial demonstrates how to preference-tune the Gemma3 1B IT model using\n",
        "[Direct Preference Optimization (DPO)](https://arxiv.org/abs/2305.18290). We\n",
        "will use the [UltraFeedback dataset](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned), a large-scale collection of\n",
        "high-quality AI feedback on user-assistant conversations.\n",
        "\n",
        "DPO is a preference tuning method for aligning large language models with\n",
        "human or AI preferences. It is a simpler and more efficient alternative\n",
        "to [RLHF](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback).\n",
        "DPO works by directly training the model on paired examples of \"chosen\"\n",
        "(preferred) and \"rejected\" responses. This process fine-tunes the model to\n",
        "increase the probability of generating desirable outputs and decrease the\n",
        "likelihood of undesirable ones, simplifying the alignment process by eliminating\n",
        "the need for reward models, complex sampling or hyperparameter tuning.\n",
        "\n",
        "This notebook has been tested on a `v6e-1` TPU instance, with 32 GB\n",
        "HBM.\n",
        "\n",
        "For reference:\n",
        "\n",
        "- Dataset: UltraFeedback ([Paper](https://arxiv.org/pdf/2310.01377), [HuggingFace](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned))\n",
        "\n",
        "- Algorithm: [Direct Preference Optimization](https://arxiv.org/pdf/2305.1829)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJPYSqpZa8QN"
      },
      "source": [
        "## Install necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --pre -q google-tunix[prod]==0.1.0.dev1\n",
        "!pip install -q huggingface-hub\n",
        "!pip install -q tensorflow\n",
        "!pip install -q tensorboardX"
      ],
      "metadata": {
        "id": "xIE95Yudc00y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p59VLus8bBXw"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-cbNiHkbAHj"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import gc\n",
        "import os\n",
        "\n",
        "from datasets import load_dataset\n",
        "from flax import nnx\n",
        "import grain\n",
        "import humanize\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import kagglehub\n",
        "import optax\n",
        "from orbax import checkpoint as ocp\n",
        "import qwix\n",
        "from tunix.examples.data import translation_dataset as data_lib\n",
        "from tunix.generate import sampler as sampler_lib\n",
        "from tunix.models.gemma3 import model as gemma3_model_lib\n",
        "from tunix.models.gemma3 import params as gemma3_params_lib\n",
        "from tunix.sft import metrics_logger\n",
        "from tunix.sft.dpo.dpo_trainer import DPOTrainer, DPOTrainingConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qICKxcEsbIh2"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Let's define the configuration we are going to use. Note that this is by no\n",
        "means a \"perfect\" set of hyperparameters. To get good results, you might have\n",
        "to train the model for longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S4dbwBxbAKL"
      },
      "outputs": [],
      "source": [
        "# ====== Data ======\n",
        "TRAIN_FRACTION = 0.75\n",
        "\n",
        "# ====== LoRA ======\n",
        "RANK = 16\n",
        "ALPHA = 2.0\n",
        "\n",
        "# ====== Sharding ======\n",
        "MESH = [(1, 1), (\"fsdp\", \"tp\")]\n",
        "MODEL_ID = \"google/gemma-3-1b-it\"\n",
        "\n",
        "# ====== DPO ======\n",
        "BETA = 0.04\n",
        "MAX_PROMPT_LENGTH = 512\n",
        "MAX_RESPONSE_LENGTH = 1024\n",
        "\n",
        "# ====== Training ======\n",
        "BATCH_SIZE = 1\n",
        "NUM_BATCHES = 100\n",
        "NUM_EPOCHS = 1  # can potentially train for more epochs\n",
        "# Number of training steps.\n",
        "MAX_STEPS = int(NUM_BATCHES * TRAIN_FRACTION * NUM_EPOCHS)\n",
        "WARMUP_STEPS = 0.1 * MAX_STEPS\n",
        "MAX_GRAD_NORM = 0.1\n",
        "EVAL_EVERY_N_STEPS = 10\n",
        "\n",
        "# === AdamW, warmup, cosine scheduler ===\n",
        "LEARNING_RATE = 3e-6\n",
        "B1 = 0.9\n",
        "B2 = 0.99\n",
        "WEIGHT_DECAY = 0.1\n",
        "\n",
        "# Checkpoint saving\n",
        "INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\n",
        "CKPT_DIR = \"/tmp/content/ckpts/\"\n",
        "SAVE_INTERVAL_STEPS = 500\n",
        "MAX_TO_KEEP = 4\n",
        "\n",
        "# ====== Generation/Inference ======\n",
        "TOTAL_GENERATION_STEPS = 1024\n",
        "\n",
        "TOP_P = 1.0\n",
        "TOP_K = 50\n",
        "TEMPERATURE = 0.9\n",
        "CACHE_SIZE = TOTAL_GENERATION_STEPS + MAX_PROMPT_LENGTH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7aZxpj-c0nQ"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNP8ZyNwbAMP"
      },
      "outputs": [],
      "source": [
        "def show_hbm_usage():\n",
        "  \"\"\"Displays memory usage per device.\"\"\"\n",
        "  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n",
        "\n",
        "  for d in jax.local_devices():\n",
        "    stats = d.memory_stats()\n",
        "    used = stats[\"bytes_in_use\"]\n",
        "    limit = stats[\"bytes_limit\"]\n",
        "    print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")\n",
        "\n",
        "\n",
        "show_hbm_usage()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV0h6l2Lc35K"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "The data preprocessing is taken care of by Tunix's `DPOTrainer`. All we need is\n",
        "to make sure we feed in our prompts in the correct format (for example, adding\n",
        "correct special tokens, such as `\u003cstart_of_turn\u003e`, `\u003cend_of_turn\u003e`), etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ-ql-rPqQ6b"
      },
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE = \"\"\"\u003cstart_of_turn\u003euser\n",
        "{prompt}\u003cend_of_turn\u003e\n",
        "\u003cstart_of_turn\u003emodel\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCIcc5EQbAOl"
      },
      "outputs": [],
      "source": [
        "def get_dataset() -\u003e grain.MapDataset:\n",
        "  hf_dataset = load_dataset(\n",
        "      \"argilla/ultrafeedback-binarized-preferences-cleaned\"\n",
        "  )[\"train\"]\n",
        "\n",
        "  def _get_response(x):\n",
        "    for element in x:\n",
        "      if element[\"role\"] == \"assistant\":\n",
        "        return element[\"content\"]\n",
        "\n",
        "  dataset = grain.MapDataset.source(hf_dataset).map(\n",
        "      lambda x: {\n",
        "          \"prompts\": PROMPT_TEMPLATE.format(prompt=x[\"prompt\"]),\n",
        "          \"chosen_responses\": _get_response(x[\"chosen\"]),\n",
        "          \"rejected_responses\": _get_response(x[\"rejected\"]),\n",
        "      }\n",
        "  )\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XotjpbJskSw"
      },
      "outputs": [],
      "source": [
        "dataset = get_dataset().batch(BATCH_SIZE)[:NUM_BATCHES]\n",
        "\n",
        "if TRAIN_FRACTION == 1.0:\n",
        "  train_dataset = dataset.repeat(NUM_EPOCHS)\n",
        "  test_dataset = None\n",
        "else:\n",
        "  train_dataset = dataset[: int(len(dataset) * TRAIN_FRACTION)]\n",
        "  train_dataset = train_dataset.repeat(NUM_EPOCHS)\n",
        "\n",
        "  test_dataset = dataset[int(len(dataset) * TRAIN_FRACTION) :].repeat(\n",
        "      NUM_EPOCHS\n",
        "  )\n",
        "\n",
        "len(train_dataset), len(test_dataset) if test_dataset is not None else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec9tx4i_vzjL"
      },
      "source": [
        "Let's see how one batch of data looks like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjen14pZs5he"
      },
      "outputs": [],
      "source": [
        "for ele in train_dataset:\n",
        "  print(ele)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEDGl6aHu-G5"
      },
      "source": [
        "## Load policy model and reference model\n",
        "\n",
        "The policy model is the model which is actually trained and whose weights are\n",
        "updated. The reference model is the model which stays fixed during\n",
        "training, and with which we compare the policy model.\n",
        "\n",
        "Typically, the reference model is the base model, and the policy model is the\n",
        "same base model, but with LoRA parameters. Only the LoRA parameters are updated.\n",
        "\n",
        "Note: We perform full precision (fp32) training. You can, however, leverage Qwix\n",
        "for QAT (Quantization Aware Training).\n",
        "\n",
        "To load the model, you need to be on [Kaggle](https://www.kaggle.com/) and need\n",
        "to have agreed to the Gemma license\n",
        "[here](https://www.kaggle.com/models/google/gemma/flax/). Instead of logging in, you can set you Kaggle credentials as secrets in your runtime environment. This way, you don't have to manually enter your username and password every time you run the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "520LpWG5tBs8"
      },
      "outputs": [],
      "source": [
        "# Log in (no need to fill this in if you've set up Colab Secrets)\n",
        "if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n",
        "  kagglehub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66l83g2TvNEH"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "ignore_patterns = [\n",
        "    \"*.pth\",  # Ignore PyTorch .pth weight files\n",
        "]\n",
        "print(f\"Downloading {MODEL_ID} from Hugging Face...\")\n",
        "ckpt_path = snapshot_download(repo_id=MODEL_ID, ignore_patterns=[\"*.pth\"])\n",
        "print(f\"Model successfully downloaded to: {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_7HIRFpwTUN"
      },
      "source": [
        "### Model Loading and LoRA Application\n",
        "\n",
        "These two functions work together to load a base model from a checkpoint and apply a LoRA (Low-Rank Adaptation) layer to it.\n",
        "\n",
        "* `get_ref_model`: Loads the complete Gemma model from a specified checkpoint path. It uses **JAX sharding** to distribute the model parameters across multiple devices.\n",
        "* `get_lora_model`: Takes the base model and applies LoRA layers to it. It uses a `LoraProvider` to select specific layers (like attention and MLP layers) to be adapted. The resulting LoRA-infused model is then sharded and updated to ensure it's ready for distributed training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdYLUAZUvNKX"
      },
      "outputs": [],
      "source": [
        "from tunix.models.gemma3 import params_safetensors as params_safetensors_lib\n",
        "\n",
        "\n",
        "def get_base_model(ckpt_path):\n",
        "  model_config = gemma3_model_lib.ModelConfig.gemma3_1b()\n",
        "  mesh = jax.make_mesh(*MESH)\n",
        "  with mesh:\n",
        "    try:\n",
        "      model = gemma3_params_lib.create_model_from_safe_tensors(\n",
        "          ckpt_path, model_config, mesh\n",
        "      )\n",
        "    except:\n",
        "      model = params_safetensors_lib.create_model_from_safe_tensors(\n",
        "          ckpt_path, model_config, mesh\n",
        "      )\n",
        "\n",
        "  return model, mesh, model_config\n",
        "\n",
        "\n",
        "def get_lora_model(base_model, mesh):\n",
        "  lora_provider = qwix.LoraProvider(\n",
        "      module_path=(\n",
        "          \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
        "          \".*attn_vec_einsum\"\n",
        "      ),\n",
        "      rank=RANK,\n",
        "      alpha=ALPHA,\n",
        "  )\n",
        "\n",
        "  model_input = base_model.get_model_input()\n",
        "  lora_model = qwix.apply_lora_to_model(\n",
        "      base_model, lora_provider, **model_input\n",
        "  )\n",
        "\n",
        "  if mesh is not None:\n",
        "    with mesh:\n",
        "      state = nnx.state(lora_model)\n",
        "      pspecs = nnx.get_partition_spec(state)\n",
        "      sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
        "      nnx.update(lora_model, sharded_state)\n",
        "\n",
        "  return lora_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3jtTMwuvNNM"
      },
      "outputs": [],
      "source": [
        "# Reference model\n",
        "ref_model, mesh, model_config = get_base_model(ckpt_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbanWbcrNDnT"
      },
      "outputs": [],
      "source": [
        "# Policy model\n",
        "lora_model = get_lora_model(ref_model, mesh=mesh)\n",
        "nnx.display(lora_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CI1bNQCNIXv"
      },
      "outputs": [],
      "source": [
        "# Tokenizer\n",
        "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
        "\n",
        "tokenizer = tokenizer_lib.Tokenizer(\n",
        "    tokenizer_type=\"huggingface\",\n",
        "    tokenizer_path=MODEL_ID,\n",
        "    add_bos=True,\n",
        "    add_eos=True,\n",
        "    hf_access_token=os.environ.get(\"HF_TOKEN\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZQyxguNNQBn"
      },
      "source": [
        "## Train\n",
        "\n",
        "Let's set up all the configs first - checkpointing, metric logging and training.\n",
        "We then train the model.\n",
        "\n",
        "Note: To get good results, it is advised to train the model for longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUgishCnNRJg"
      },
      "outputs": [],
      "source": [
        "# Ckpt saving\n",
        "checkpointing_options = ocp.CheckpointManagerOptions(\n",
        "    save_interval_steps=SAVE_INTERVAL_STEPS, max_to_keep=MAX_TO_KEEP\n",
        ")\n",
        "\n",
        "# Metrics logger\n",
        "metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
        "    log_dir=\"/tmp/content/tmp/tensorboard/dpo\", flush_every_n_steps=20\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH6SxKLTNXGv"
      },
      "outputs": [],
      "source": [
        "# Logs\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /tmp/content/tmp/tensorboard/dpo --port=0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer, learning rate scheduler, gradient clipping\n",
        "optimizer = optax.adamw(\n",
        "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
        "        init_value=0.0,\n",
        "        peak_value=LEARNING_RATE,\n",
        "        warmup_steps=WARMUP_STEPS,\n",
        "        decay_steps=MAX_STEPS,\n",
        "        end_value=0.0,\n",
        "    ),\n",
        "    b1=B1,\n",
        "    b2=B2,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        ")\n",
        "if MAX_GRAD_NORM is not None:\n",
        "  optimizer = optax.chain(\n",
        "      optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n",
        "      optimizer,\n",
        "  )"
      ],
      "metadata": {
        "id": "Yf_s45E1CVoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dgdLMBONZWq"
      },
      "outputs": [],
      "source": [
        "# Training config\n",
        "training_config = DPOTrainingConfig(\n",
        "    beta=BETA,\n",
        "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    metrics_logging_options=metrics_logging_options,\n",
        "    checkpoint_root_directory=CKPT_DIR,\n",
        "    checkpointing_options=checkpointing_options,\n",
        "    max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "    max_response_length=MAX_RESPONSE_LENGTH,\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model=lora_model,\n",
        "    ref_model=ref_model,\n",
        "    optimizer=optimizer,\n",
        "    tokenizer=tokenizer,\n",
        "    training_config=training_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyB1JSsBNZZd"
      },
      "outputs": [],
      "source": [
        "if mesh is None:\n",
        "  dpo_trainer.train(train_dataset)\n",
        "else:\n",
        "  with mesh:\n",
        "    dpo_trainer.train(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPsYuV9bQWEW"
      },
      "source": [
        "## Evaluate\n",
        "\n",
        "We evaluate the model's performance on the test dataset. For this initial analysis, we perform a qualitative comparison using randomly selected examples to get a sense of the output quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X341rppiN-kA"
      },
      "outputs": [],
      "source": [
        "def generate(prompt, sampler, temperature=1.0, top_k=64, top_p=0.95):\n",
        "  \"\"\"Given prompt, generates text.\"\"\"\n",
        "\n",
        "  input_batch = [PROMPT_TEMPLATE.format(prompt=prompt)]\n",
        "\n",
        "  out_data = sampler(\n",
        "      input_strings=input_batch,\n",
        "      max_generation_steps=TOTAL_GENERATION_STEPS,\n",
        "      max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "      temperature=temperature,\n",
        "      top_k=top_k,\n",
        "      top_p=top_p,\n",
        "      echo=False,\n",
        "  )\n",
        "\n",
        "  output = out_data.text\n",
        "\n",
        "  if isinstance(prompt, str):\n",
        "    return output[0]\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsKlcGjAQbq-"
      },
      "outputs": [],
      "source": [
        "# Load the checkpoint first.\n",
        "trained_ckpt_path = os.path.join(CKPT_DIR, str(MAX_STEPS), \"model_params\")\n",
        "\n",
        "abs_params = jax.tree.map(\n",
        "    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
        "    nnx.state(lora_model, nnx.LoRAParam),\n",
        ")\n",
        "checkpointer = ocp.StandardCheckpointer()\n",
        "trained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n",
        "\n",
        "nnx.update(\n",
        "    lora_model,\n",
        "    jax.tree.map(\n",
        "        lambda a, b: b,\n",
        "        nnx.state(lora_model, nnx.LoRAParam),\n",
        "        trained_lora_params,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3Ev9CCQQvBi"
      },
      "outputs": [],
      "source": [
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_model,\n",
        "    tokenizer=tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=CACHE_SIZE + 256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lr1UWcmREHz"
      },
      "outputs": [],
      "source": [
        "# Randomly select an example from test dataset and eyeball compare the model\n",
        "# output vs. chosen and reject responses.\n",
        "\n",
        "test_index = 20\n",
        "print(\"prompt: \\n\\n\", test_dataset[test_index][\"prompts\"])\n",
        "print(\"==\" * 10)\n",
        "print(\"chosen: \\n\\n\", test_dataset[test_index][\"chosen_responses\"])\n",
        "print(\"==\" * 10)\n",
        "print(\"rejected: \\n\\n\", test_dataset[test_index][\"rejected_responses\"])\n",
        "print(\"==\" * 10)\n",
        "print(\"DPO tuned model output\")\n",
        "text = generate(\n",
        "    prompt=test_dataset[test_index][\"prompts\"],\n",
        "    sampler=sampler,\n",
        "    temperature=TEMPERATURE,\n",
        "    top_k=TOP_K,\n",
        "    top_p=TOP_P,\n",
        ")\n",
        "print(text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
