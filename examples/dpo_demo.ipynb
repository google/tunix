{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNiTzJRoa1hf"
      },
      "source": [
        "This tutorial demonstrates how to preference-tune a Gemma model using Direct\n",
        "Preference Optimization (DPO). We will use the UltraFeedback dataset, a\n",
        "large-scale collection of high-quality AI feedback on user-assistant\n",
        "conversations.\n",
        "\n",
        "DPO is a preference tuning method for aligning large language models with\n",
        "human or AI preferences. It is a more efficient, performant alternative\n",
        "to RLHF. DPO works by directly training the model on paired examples of \"chosen\"\n",
        "(preferred) and \"rejected\" responses. This process fine-tunes the model to\n",
        "increase the probability of generating desirable outputs and decrease the\n",
        "likelihood of undesirable ones, simplifying the alignment process by eliminating\n",
        "the need for complex sampling or hyperparameter tuning.\n",
        "\n",
        "This notebook has been tested on Colab's `v6e-1` TPU instance, with 30 GB\n",
        "memory.\n",
        "\n",
        "For reference:\n",
        "\n",
        "- Dataset: UltraFeedback ([Paper](https://arxiv.org/pdf/2310.01377), [HuggingFace](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned))\n",
        "\n",
        "- Algorithm: [Direct Preference Optimization](https://arxiv.org/pdf/2305.1829)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJPYSqpZa8QN"
      },
      "source": [
        "## Install necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZ2u9BFXat5c"
      },
      "outputs": [],
      "source": [
        "!pip install -q kagglehub\n",
        "\n",
        "!pip install -q ipywidgets\n",
        "\n",
        "!pip install -q datasets\n",
        "!pip install -q tensorflow\n",
        "!pip install -q tensorboardX\n",
        "!pip install -q transformers\n",
        "!pip install -q grain\n",
        "!pip install -q git+https://github.com/google/tunix\n",
        "!pip install -q git+https://github.com/google/qwix\n",
        "\n",
        "!pip uninstall -q -y flax\n",
        "!pip install -q git+https://github.com/google/flax.git\n",
        "\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p59VLus8bBXw"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-cbNiHkbAHj"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import gc\n",
        "import os\n",
        "\n",
        "from datasets import load_dataset\n",
        "from flax import nnx\n",
        "import grain\n",
        "import humanize\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import kagglehub\n",
        "import optax\n",
        "from orbax import checkpoint as ocp\n",
        "import qwix\n",
        "from tunix.examples.data import translation_dataset as data_lib\n",
        "from tunix.generate import sampler as sampler_lib\n",
        "from tunix.models.gemma3 import params as gemma3_params_lib\n",
        "from tunix.sft import metrics_logger\n",
        "from tunix.sft.dpo.dpo_trainer import DPOTrainer, DPOTrainingConfig\n",
        "from tunix.sft.peft_main import obtain_model_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qICKxcEsbIh2"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Let's define the configuration we are going to use. Note that this is by no\n",
        "means a \"perfect\" set of hyperparameters. To get good results, you might have\n",
        "to train the model for longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S4dbwBxbAKL"
      },
      "outputs": [],
      "source": [
        "# ====== Data ======\n",
        "TRAIN_FRACTION = 0.75\n",
        "\n",
        "# ====== LoRA ======\n",
        "RANK = 16\n",
        "ALPHA = 2.0\n",
        "\n",
        "# ====== Sharding ======\n",
        "MESH = None\n",
        "\n",
        "# ====== DPO ======\n",
        "BETA = 0.04\n",
        "MAX_PROMPT_LENGTH = 512\n",
        "MAX_RESPONSE_LENGTH = 1024\n",
        "\n",
        "# ====== Training ======\n",
        "BATCH_SIZE = 1\n",
        "NUM_BATCHES = 100\n",
        "NUM_EPOCHS = 2  # can potentially train for more epochs\n",
        "# Number of training steps.\n",
        "MAX_STEPS = int(NUM_BATCHES * TRAIN_FRACTION * NUM_EPOCHS)\n",
        "EVAL_EVERY_N_STEPS = 10\n",
        "\n",
        "# === AdamW, warmup, cosine scheduler ===\n",
        "LEARNING_RATE = 3e-6\n",
        "B1 = 0.9\n",
        "B2 = 0.99\n",
        "WEIGHT_DECAY = 0.1\n",
        "\n",
        "# Checkpoint saving\n",
        "INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\n",
        "CKPT_DIR = \"/tmp/content/ckpts/\"\n",
        "SAVE_INTERVAL_STEPS = 500\n",
        "MAX_TO_KEEP = 4\n",
        "\n",
        "# ====== Generation/Inference ======\n",
        "TOTAL_GENERATION_STEPS = 1024\n",
        "\n",
        "TOP_P = 1.0\n",
        "TOP_K = 50\n",
        "TEMPERATURE = 0.9\n",
        "CACHE_SIZE = TOTAL_GENERATION_STEPS + MAX_PROMPT_LENGTH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7aZxpj-c0nQ"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNP8ZyNwbAMP"
      },
      "outputs": [],
      "source": [
        "def show_hbm_usage():\n",
        "  \"\"\"Displays memory usage per device.\"\"\"\n",
        "  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n",
        "\n",
        "  for d in jax.local_devices():\n",
        "    stats = d.memory_stats()\n",
        "    used = stats[\"bytes_in_use\"]\n",
        "    limit = stats[\"bytes_limit\"]\n",
        "    print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")\n",
        "\n",
        "\n",
        "show_hbm_usage()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV0h6l2Lc35K"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "The data preprocessing is taken care of by Tunix's `DPOTrainer`. All we need is\n",
        "to make sure we feed in our prompts in the correct format (for example, adding\n",
        "correct special tokens, such as `\u003cstart_of_turn\u003e`, `\u003cend_of_turn\u003e`), etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ-ql-rPqQ6b"
      },
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE = \"\"\"\u003cstart_of_turn\u003euser\n",
        "{prompt}\u003cend_of_turn\u003e\n",
        "\u003cstart_of_turn\u003emodel\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCIcc5EQbAOl"
      },
      "outputs": [],
      "source": [
        "def get_dataset() -\u003e grain.MapDataset:\n",
        "  hf_dataset = load_dataset(\n",
        "      \"argilla/ultrafeedback-binarized-preferences-cleaned\"\n",
        "  )[\"train\"]\n",
        "\n",
        "  def _get_response(x):\n",
        "    for element in x:\n",
        "      if element[\"role\"] == \"assistant\":\n",
        "        return element[\"content\"]\n",
        "\n",
        "  dataset = grain.MapDataset.source(hf_dataset).map(\n",
        "      lambda x: {\n",
        "          \"prompts\": PROMPT_TEMPLATE.format(prompt=x[\"prompt\"]),\n",
        "          \"chosen_responses\": _get_response(x[\"chosen\"]),\n",
        "          \"rejected_responses\": _get_response(x[\"rejected\"]),\n",
        "      }\n",
        "  )\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XotjpbJskSw"
      },
      "outputs": [],
      "source": [
        "dataset = get_dataset().batch(BATCH_SIZE)[:NUM_BATCHES]\n",
        "\n",
        "if TRAIN_FRACTION == 1.0:\n",
        "  train_dataset = dataset.repeat(NUM_EPOCHS)\n",
        "  test_dataset = None\n",
        "else:\n",
        "  train_dataset = dataset[: int(len(dataset) * TRAIN_FRACTION)]\n",
        "  train_dataset = train_dataset.repeat(NUM_EPOCHS)\n",
        "\n",
        "  test_dataset = dataset[int(len(dataset) * TRAIN_FRACTION) :].repeat(\n",
        "      NUM_EPOCHS\n",
        "  )\n",
        "\n",
        "len(train_dataset), len(test_dataset) if test_dataset is not None else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec9tx4i_vzjL"
      },
      "source": [
        "Let's see how one batch of data looks like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjen14pZs5he"
      },
      "outputs": [],
      "source": [
        "for ele in train_dataset:\n",
        "  print(ele)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEDGl6aHu-G5"
      },
      "source": [
        "## Load policy model and reference model\n",
        "\n",
        "The policy model is the model which is actually trained and whose weights are\n",
        "updated. The reference model is the model which stays fixed during\n",
        "training, and with which we compare the policy model with.\n",
        "\n",
        "Typically, the reference model is the base model, and the policy model is the\n",
        "same base model, but with LoRA parameters. Only the LoRA parameters are updated.\n",
        "\n",
        "Note: We perform full precision (fp32) training. You can, however, leverage Qwix for QAT.\n",
        "\n",
        "To load the model, you need to be on [Kaggle](https://www.kaggle.com/) and need\n",
        "to have agreed to the Gemma license\n",
        "[here](https://www.kaggle.com/models/google/gemma/flax/). Instead of logging in, we recommend using Colab Secrets. This way, you don't have to manually enter your username and password every time you run the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "520LpWG5tBs8"
      },
      "outputs": [],
      "source": [
        "# Log in (no need to fill this in if you've set up Colab Secrets)\n",
        "if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n",
        "  kagglehub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66l83g2TvNEH"
      },
      "outputs": [],
      "source": [
        "model_path = {\n",
        "    \"gemma3\": \"google/gemma-3/flax/\",\n",
        "}\n",
        "model_family = \"gemma3\"\n",
        "model_params = \"gemma3-1b\"\n",
        "model_version = \"gemma3-1b-it\"\n",
        "\n",
        "print(f\"{model_path[model_family]}{model_version}\")\n",
        "\n",
        "kaggle_ckpt_path = kagglehub.model_download(\n",
        "    f\"{model_path[model_family]}{model_version}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_7HIRFpwTUN"
      },
      "source": [
        "### Model Loading and LoRA Application\n",
        "\n",
        "These two functions work together to load a base model from a checkpoint and apply a LoRA (Low-Rank Adaptation) layer to it.\n",
        "\n",
        "* `get_ref_model`: Loads the complete Gemma model from a specified checkpoint path. It uses **JAX sharding** to distribute the model parameters across multiple devices.\n",
        "* `get_lora_model`: Takes the base model and applies LoRA layers to it. It uses a `LoraProvider` to select specific layers (like attention and MLP layers) to be adapted. The resulting LoRA-infused model is then sharded and updated to ensure it's ready for distributed training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdYLUAZUvNKX"
      },
      "outputs": [],
      "source": [
        "def get_ref_model(kaggle_ckpt_path):\n",
        "  mesh = None\n",
        "  if MESH is not None:\n",
        "    mesh = jax.make_mesh(*MESH)\n",
        "\n",
        "  model_config = obtain_model_config(model_params)\n",
        "  ref_model = gemma3_params_lib.create_model_from_checkpoint(\n",
        "      os.path.join(kaggle_ckpt_path, model_version), model_config, MESH\n",
        "  )\n",
        "  return ref_model, mesh, model_config\n",
        "\n",
        "\n",
        "def get_lora_model(base_model, mesh):\n",
        "  lora_provider = qwix.LoraProvider(\n",
        "      module_path=(\n",
        "          \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
        "          \".*attn_vec_einsum\"\n",
        "      ),\n",
        "      rank=RANK,\n",
        "      alpha=ALPHA,\n",
        "  )\n",
        "\n",
        "  model_input = base_model.get_model_input()\n",
        "  lora_model = qwix.apply_lora_to_model(\n",
        "      base_model, lora_provider, **model_input\n",
        "  )\n",
        "\n",
        "  if mesh is not None:\n",
        "    with mesh:\n",
        "      state = nnx.state(lora_model)\n",
        "      pspecs = nnx.get_partition_spec(state)\n",
        "      sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
        "      nnx.update(lora_model, sharded_state)\n",
        "\n",
        "  return lora_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3jtTMwuvNNM"
      },
      "outputs": [],
      "source": [
        "# Reference model\n",
        "ref_model, mesh, model_config = get_ref_model(kaggle_ckpt_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbanWbcrNDnT"
      },
      "outputs": [],
      "source": [
        "# Policy model\n",
        "lora_model = get_lora_model(ref_model, mesh=mesh)\n",
        "nnx.display(lora_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CI1bNQCNIXv"
      },
      "outputs": [],
      "source": [
        "# Tokenizer\n",
        "tokenizer = data_lib.GemmaTokenizer(\n",
        "    os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZQyxguNNQBn"
      },
      "source": [
        "## Train\n",
        "\n",
        "Let's set up all the configs first - checkpointing, metric logging and training.\n",
        "We then train the model.\n",
        "\n",
        "Note: To get good results, it is advised to train the model for longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUgishCnNRJg"
      },
      "outputs": [],
      "source": [
        "# Ckpt saving\n",
        "checkpointing_options = ocp.CheckpointManagerOptions(\n",
        "    save_interval_steps=SAVE_INTERVAL_STEPS, max_to_keep=MAX_TO_KEEP\n",
        ")\n",
        "\n",
        "# Metrics logger\n",
        "metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
        "    log_dir=\"/tmp/content/tmp/tensorboard/dpo\", flush_every_n_steps=20\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH6SxKLTNXGv"
      },
      "outputs": [],
      "source": [
        "# Logs\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /tmp/content/tmp/tensorboard/dpo --port=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dgdLMBONZWq"
      },
      "outputs": [],
      "source": [
        "# Training config\n",
        "training_config = DPOTrainingConfig(\n",
        "    beta=BETA,\n",
        "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    metrics_logging_options=metrics_logging_options,\n",
        "    checkpoint_root_directory=CKPT_DIR,\n",
        "    checkpointing_options=checkpointing_options,\n",
        "    max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "    max_response_length=MAX_RESPONSE_LENGTH,\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model=lora_model,\n",
        "    ref_model=ref_model,\n",
        "    optimizer=optax.adamw(\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        b1=B1,\n",
        "        b2=B2,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "    ),\n",
        "    tokenizer=tokenizer,\n",
        "    training_config=training_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyB1JSsBNZZd"
      },
      "outputs": [],
      "source": [
        "if mesh is None:\n",
        "  dpo_trainer.train(train_dataset)\n",
        "else:\n",
        "  with mesh:\n",
        "    dpo_trainer.train(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPsYuV9bQWEW"
      },
      "source": [
        "## Evaluate\n",
        "\n",
        "We evaluate the model's performance on the test dataset. For this initial analysis, we perform a qualitative comparison using randomly selected examples to get a sense of the output quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X341rppiN-kA"
      },
      "outputs": [],
      "source": [
        "def generate(prompt, sampler, temperature=1.0, top_k=64, top_p=0.95):\n",
        "  \"\"\"Given prompt, generates text.\"\"\"\n",
        "\n",
        "  input_batch = [PROMPT_TEMPLATE.format(prompt=prompt)]\n",
        "\n",
        "  out_data = sampler(\n",
        "      input_strings=input_batch,\n",
        "      max_generation_steps=TOTAL_GENERATION_STEPS,\n",
        "      max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "      temperature=temperature,\n",
        "      top_k=top_k,\n",
        "      top_p=top_p,\n",
        "      echo=False,\n",
        "  )\n",
        "\n",
        "  output = out_data.text\n",
        "\n",
        "  if isinstance(prompt, str):\n",
        "    return output[0]\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsKlcGjAQbq-"
      },
      "outputs": [],
      "source": [
        "# Load the checkpoint first.\n",
        "trained_ckpt_path = os.path.join(CKPT_DIR, str(MAX_STEPS), \"model_params\")\n",
        "\n",
        "abs_params = jax.tree.map(\n",
        "    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
        "    nnx.state(lora_model, nnx.LoRAParam),\n",
        ")\n",
        "checkpointer = ocp.StandardCheckpointer()\n",
        "trained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n",
        "\n",
        "nnx.update(\n",
        "    lora_model,\n",
        "    jax.tree.map(\n",
        "        lambda a, b: b,\n",
        "        nnx.state(lora_model, nnx.LoRAParam),\n",
        "        trained_lora_params,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3Ev9CCQQvBi"
      },
      "outputs": [],
      "source": [
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_model,\n",
        "    tokenizer=tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=CACHE_SIZE + 256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lr1UWcmREHz"
      },
      "outputs": [],
      "source": [
        "# Randomly select an example from test dataset and eyeball compare the model\n",
        "# output vs. chosen and reject responses.\n",
        "\n",
        "test_index = 20\n",
        "print(\"prompt: \\n\\n\", test_dataset[test_index][\"prompt\"])\n",
        "print(\"==\" * 10)\n",
        "print(\"chosen: \\n\\n\", test_dataset[test_index][\"chosen\"])\n",
        "print(\"==\" * 10)\n",
        "print(\"rejected: \\n\\n\", test_dataset[test_index][\"rejected\"])\n",
        "print(\"==\" * 10)\n",
        "print(\"DPO tuned model output\")\n",
        "text = generate(\n",
        "    prompt=test_dataset[test_index][\"prompt\"],\n",
        "    sampler=sampler,\n",
        "    temperature=TEMPERATURE,\n",
        "    top_k=TOP_K,\n",
        "    top_p=TOP_P,\n",
        ")\n",
        "print(text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
