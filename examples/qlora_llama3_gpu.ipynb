{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvQWlinG9D24"
      },
      "source": [
        "# Parameter-Efficient Fine-Tuning of Llama 3.1-8B with LoRA/QLoRA on NVIDIA GPUs using JAX and Tunix\n",
        "\n",
        "This tutorial walks you through parameter-efficient fine-tuning (PEFT) of Llama 3.1-8B using LoRA and QLoRA on NVIDIA GPUs with JAX, Tunix, and Qwix. Unlike full-parameter SFT, PEFT freezes the base model weights and trains only small adapter matrices, dramatically reducing memory requirements and training time while maintaining model quality.\n",
        "\n",
        "**What you'll do:**\n",
        "1. Set up the environment and authenticate with Hugging Face\n",
        "2. Load the Llama 3.1-8B base model and apply LoRA/QLoRA adapters\n",
        "3. Prepare the UltraChat 200k dataset for instruction fine-tuning\n",
        "4. Configure and run parameter-efficient fine-tuning\n",
        "5. Visualize training metrics with TensorBoard\n",
        "6. Run a quick inference sanity check\n",
        "\n",
        "## Preliminaries\n",
        "\n",
        "### Make sure you have supported hardware\n",
        "\n",
        "**Hardware requirements.** QLoRA with 4-bit quantization can fine-tune Llama 3.1-8B on a single GPU with **16 GB+ of VRAM**. For LoRA without quantization, 24 GB+ is recommended. Multiple GPUs enable larger batch sizes and faster training through data parallelism; on multi-GPU systems, the model is automatically sharded across devices using FSDP and tensor parallelism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hq8dftyo9D25"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUjQZdn09D25"
      },
      "source": [
        "### Set your Hugging Face token\n",
        "\n",
        "Create a [Hugging Face](https://huggingface.co/) access token in your Hugging Face account [settings](https://huggingface.co/settings/tokens), copy it, and paste it into the field below. This token is required to authenticate with the Hugging Face Hub and download the Llama 3.1 model and related assets; once saved, it will be reused by this environment for the rest of the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3tA6_VZ9D25"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from ipywidgets import Password, Button, HBox, Output\n",
        "from IPython.display import display\n",
        "\n",
        "try:\n",
        "    from huggingface_hub import whoami\n",
        "except Exception:\n",
        "    from huggingface_hub import HfApi\n",
        "\n",
        "def _verify_token(token: str) -> str:\n",
        "    try:\n",
        "        return whoami(token=token).get(\"name\", \"unknown\")\n",
        "    except TypeError:\n",
        "        return HfApi(token=token).whoami().get(\"name\", \"unknown\")\n",
        "\n",
        "token_box = Password(description=\"HF Token:\", placeholder=\"paste your token here\", layout={\"width\": \"400px\"})\n",
        "save_btn = Button(description=\"Save\", button_style=\"success\")\n",
        "out = Output()\n",
        "\n",
        "def save_token(_):\n",
        "    out.clear_output()\n",
        "    with out:\n",
        "        existing = os.environ.get(\"HF_TOKEN\")\n",
        "        entered = token_box.value.strip()\n",
        "        if existing and not entered:\n",
        "            user = _verify_token(existing)\n",
        "            print(f\"Using existing HF_TOKEN. Logged in as: {user}\")\n",
        "            return\n",
        "        if not entered:\n",
        "            print(\"No HF token entered.\")\n",
        "            return\n",
        "        os.environ[\"HF_TOKEN\"] = entered\n",
        "        user = _verify_token(entered)\n",
        "        print(f\"Token saved. Logged in as: {user}\")\n",
        "\n",
        "save_btn.on_click(save_token)\n",
        "display(HBox([token_box, save_btn]), out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeIltbKB9D25"
      },
      "source": [
        "### Authenticate with Hugging Face\n",
        "\n",
        "Verify that your Hugging Face token is set and valid. If the token is missing, an error is raised immediately rather than failing silently during model download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zM4Hbqo9D25"
      },
      "outputs": [],
      "source": [
        "# Prefer environment variable if already set\n",
        "\n",
        "from huggingface_hub.v1.hf_api import whoami\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
        "\n",
        "if HF_TOKEN:\n",
        "    try:\n",
        "        user = whoami()[\"name\"]\n",
        "        print(f\"Authenticated with Hugging Face as: {user} (via HF_TOKEN env)\")\n",
        "    except Exception as e:\n",
        "        print(\"HF_TOKEN is set but authentication failed:\", e)\n",
        "else:\n",
        "    raise RuntimeError(\n",
        "        \"HF_TOKEN is not set. Please create a Hugging Face access token \"\n",
        "        \"and export it as an environment variable.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tgyp9oZG9D26"
      },
      "source": [
        "### Acquire permission to use the gated model\n",
        "\n",
        "Llama 3.1-8B is a gated model, so you must explicitly request access before it can be downloaded. Visit the [model page](https://huggingface.co/meta-llama/Llama-3.1-8B) on Hugging Face, log in with the same account linked to your access token, and click **Request access**. You'll need to agree to Meta's license terms; approval is usually granted quickly but is not automatic. Once approved, your Hugging Face token will authorize downloads transparently. If you skip this step, model downloads will fail even with a valid token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBDdFFp39D26"
      },
      "source": [
        "### Set up the environment\n",
        "\n",
        "### Import dependencies\n",
        "\n",
        "Import the core libraries needed for training:\n",
        "- **JAX/Flax**: High-performance ML framework with automatic differentiation and XLA compilation\n",
        "- **Optax**: Gradient processing and optimization library for JAX\n",
        "- **Transformers**: Hugging Face library for tokenizers and model configurations\n",
        "- **Qwix**: Quantization and LoRA utilities for JAX models\n",
        "- **Tunix**: Training utilities including `PeftTrainer` and `AutoModel` for streamlined fine-tuning\n",
        "\n",
        "The easiest way to get a working environment is the [NVIDIA NGC JAX container](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/jax), which ships with all dependencies preinstalled. To install the dependencies manually:\n",
        "\n",
        "```bash\n",
        "pip install 'jax[cuda13]' flax optax transformers datasets qwix\n",
        "```\n",
        "\n",
        "On top of the installation (either container or manual), you will need Tunix:\n",
        "\n",
        "```bash\n",
        "pip install tunix\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mmk-LGe9D26"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import time\n",
        "import shutil\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "from flax import nnx\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "\n",
        "import qwix\n",
        "from tunix.models.automodel import AutoModel\n",
        "from tunix.sft import peft_trainer, metrics_logger\n",
        "\n",
        "print(f\"JAX {jax.__version__} | Devices: {jax.devices()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz9sizzf9D26"
      },
      "source": [
        "### Create the device mesh\n",
        "\n",
        "JAX uses a device mesh to define how computation and data are distributed across GPUs. The mesh assigns logical axis names to physical device dimensions, enabling FSDP (Fully Sharded Data Parallel) and TP (Tensor Parallel) strategies. The configuration adapts automatically based on available GPUs:\n",
        "\n",
        "| GPUs | Mesh Shape | Strategy |\n",
        "|------|------------|----------|\n",
        "| 8+ | `(1, 4, 2)` | data + FSDP + TP |\n",
        "| 2â€“7 | `(N, 1)` | FSDP only |\n",
        "| 1 | `(1, 1)` | No sharding |\n",
        "\n",
        "The `fsdp` axis shards model parameters across devices to reduce per-device memory, while `tp` enables tensor-parallel splitting of large weight matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIjUzsbH9D26"
      },
      "outputs": [],
      "source": [
        "# Create mesh for sharding\n",
        "NUM_DEVICES = jax.local_device_count()\n",
        "\n",
        "if NUM_DEVICES >= 8:\n",
        "    mesh = jax.make_mesh((1, 4, 2), (\"data\", \"fsdp\", \"tp\"),\n",
        "        axis_types=(jax.sharding.AxisType.Auto,) * 3)\n",
        "elif NUM_DEVICES >= 2:\n",
        "    # Shard model across GPUs using FSDP\n",
        "    mesh = jax.make_mesh((NUM_DEVICES, 1), (\"fsdp\", \"tp\"),\n",
        "        axis_types=(jax.sharding.AxisType.Auto,) * 2)\n",
        "else:\n",
        "    # Single GPU - no sharding, but keep axis names for API consistency\n",
        "    mesh = jax.make_mesh((1, 1), (\"fsdp\", \"tp\"),\n",
        "        axis_types=(jax.sharding.AxisType.Auto,) * 2)\n",
        "\n",
        "print(f\"Devices: {NUM_DEVICES} | Mesh: {mesh.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKRr2NLd9D27"
      },
      "source": [
        "## Define model and training parameters\n",
        "\n",
        "All training hyperparameters are defined in one place for easy experimentation. The key parameters control model selection, LoRA configuration, quantization, batch size, sequence length, and training duration. Set `CLEAN_START = True` to remove existing checkpoints before training, or `False` to resume from a previous run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WkC8SGk9D27"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Suppress CUDA/TF warnings\n",
        "\n",
        "MODEL_ID = \"meta-llama/Llama-3.1-8B\"\n",
        "TOKENIZER_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "LORA_RANK = 16\n",
        "LORA_ALPHA = 32.0\n",
        "USE_QUANTIZATION = True  # set True for QLoRA (4-bit), set False for regular LoRA\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "MAX_SEQ_LENGTH = 512\n",
        "LEARNING_RATE = 1e-4\n",
        "MAX_STEPS = 100\n",
        "\n",
        "OUTPUT_DIR = \"/workspace/llama3_lora_output\"\n",
        "CLEAN_START = True  # Set to False to resume from checkpoint\n",
        "\n",
        "if CLEAN_START and os.path.exists(f\"{OUTPUT_DIR}/checkpoints\"):\n",
        "    shutil.rmtree(f\"{OUTPUT_DIR}/checkpoints\")\n",
        "    print(\"Removed old checkpoints (CLEAN_START=True)\")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcKtJRBn9D27"
      },
      "source": [
        "## Load the model\n",
        "\n",
        "### Load the tokenizer\n",
        "\n",
        "Load the tokenizer from the Instruct model variant, which includes the chat template for formatting conversations. The pad token is set to the EOS token if not already defined, which is standard for decoder-only models like Llama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8D7BAvQm9D27"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(TOKENIZER_ID, token=HF_TOKEN)\n",
        "tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token\n",
        "print(f\"Tokenizer loaded: {TOKENIZER_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDq9ya169D27"
      },
      "source": [
        "### Load the base model\n",
        "\n",
        "`AutoModel.from_pretrained()` handles the complete model loading pipeline: downloading weights from the Hugging Face Hub (cached in `model_download_path`), converting them to JAX-compatible format, and initializing the model architecture with proper sharding across the mesh.\n",
        "\n",
        "The model is loaded within the mesh context to ensure parameters are distributed correctly across devices from the start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY6G3z6q9D27"
      },
      "outputs": [],
      "source": [
        "# Load model using AutoModel\n",
        "print(f\"Loading {MODEL_ID}...\")\n",
        "load_start = time.time()\n",
        "\n",
        "with mesh:\n",
        "    base_model, model_path = AutoModel.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        mesh,\n",
        "        model_download_path=\"/hf_cache\",\n",
        "    )\n",
        "\n",
        "print(f\"Model loaded in {time.time() - load_start:.1f}s\")\n",
        "print(f\"Model path: {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUI18KNI9D27"
      },
      "source": [
        "## Apply LoRA / QLoRA\n",
        "\n",
        "Low-Rank Adaptation (LoRA) freezes the base model weights and injects small trainable matrices into attention and MLP layers. This dramatically reduces the number of trainable parameters while preserving model quality.\n",
        "\n",
        "**QLoRA** adds 4-bit NF4 quantization on top of LoRA to further reduce memory:\n",
        "- Base weights are quantized to 4-bit NormalFloat format\n",
        "- Only the small LoRA adapter weights remain in full precision\n",
        "- `tile_size=32` controls the quantization block size (must divide the smallest weight dimension)\n",
        "\n",
        "**Target modules** specify which layers receive LoRA adapters using regex patterns matching attention projections (`q_proj`, `k_proj`, `v_proj`, `o_proj`) and MLP layers (`gate_proj`, `up_proj`, `down_proj`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGF2AwBb9D27"
      },
      "outputs": [],
      "source": [
        "# Apply QLoRA / LoRA\n",
        "target_modules = \".*q_proj|.*k_proj|.*v_proj|.*o_proj|.*gate_proj|.*up_proj|.*down_proj\"\n",
        "\n",
        "lora_provider = qwix.LoraProvider(\n",
        "    module_path=target_modules,\n",
        "    rank=LORA_RANK,\n",
        "    alpha=LORA_ALPHA,\n",
        "    weight_qtype=\"nf4\" if USE_QUANTIZATION else None,\n",
        "    tile_size=32 if USE_QUANTIZATION else None,\n",
        ")\n",
        "\n",
        "dummy_input = {\n",
        "    'input_tokens': jnp.ones((1, 128), dtype=jnp.int32),\n",
        "    'positions': jnp.arange(128)[None, :],\n",
        "    'cache': None,\n",
        "    'attention_mask': jnp.ones((1, 128, 128), dtype=jnp.bool_),\n",
        "}\n",
        "\n",
        "print(f\"Applying {'QLoRA' if USE_QUANTIZATION else 'LoRA'} (rank={LORA_RANK})...\")\n",
        "lora_model = qwix.apply_lora_to_model(\n",
        "    base_model, lora_provider,\n",
        "    rngs=nnx.Rngs(params=0),  # For reproducible LoRA weight initialization\n",
        "    **dummy_input\n",
        ")\n",
        "\n",
        "with mesh:\n",
        "    state = nnx.state(lora_model)\n",
        "    sharded = jax.lax.with_sharding_constraint(state, nnx.get_partition_spec(state))\n",
        "    nnx.update(lora_model, sharded)\n",
        "\n",
        "print(f\"{'QLoRA' if USE_QUANTIZATION else 'LoRA'} applied!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUdjvwH79D27"
      },
      "source": [
        "## Prepare the training data\n",
        "\n",
        "Load the [UltraChat 200k](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k) dataset, a large collection of multi-turn conversations commonly used for instruction fine-tuning. For this tutorial, a subset of 2,000 training and 200 evaluation examples is used.\n",
        "\n",
        "The data processing pipeline applies the chat template to format conversations with special tokens, tokenizes with padding to `MAX_SEQ_LENGTH`, and creates attention masks to ignore padding tokens. Training uses an infinite generator that cycles through the data, while evaluation uses a finite iterator that yields exactly one pass through the eval set. Batch size is scaled by `NUM_DEVICES` for data parallelism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcecSYUs9D27"
      },
      "outputs": [],
      "source": [
        "# Prepare dataset\n",
        "dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft\").select(range(2000))\n",
        "eval_dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"test_sft\").select(range(200))\n",
        "\n",
        "def tokenize(ex):\n",
        "    text = tokenizer.apply_chat_template(ex[\"messages\"], tokenize=False)\n",
        "    tok = tokenizer(text, max_length=MAX_SEQ_LENGTH, padding=\"max_length\", truncation=True)\n",
        "    return {\"input_tokens\": np.array(tok[\"input_ids\"]), \"input_mask\": np.array(tok[\"attention_mask\"], dtype=bool)}\n",
        "\n",
        "train_data = [tokenize(ex) for ex in dataset]\n",
        "eval_data = [tokenize(ex) for ex in eval_dataset]\n",
        "\n",
        "# Infinite generator for training (cycles through data)\n",
        "def train_batches(data, bs):\n",
        "    i = 0\n",
        "    while True:\n",
        "        batch = data[i:i+bs] if i+bs <= len(data) else data[:bs]\n",
        "        yield {k: np.stack([x[k] for x in batch]) for k in batch[0]}\n",
        "        i = (i + bs) % len(data)\n",
        "\n",
        "# Reusable eval dataset - returns fresh finite iterator each time\n",
        "class EvalDataset:\n",
        "    def __init__(self, data, bs):\n",
        "        self.data = data\n",
        "        self.bs = bs\n",
        "    def __iter__(self):\n",
        "        for i in range(0, len(self.data), self.bs):\n",
        "            batch = self.data[i:i+self.bs]\n",
        "            if len(batch) == self.bs:\n",
        "                yield {k: np.stack([x[k] for x in batch]) for k in batch[0]}\n",
        "\n",
        "train_ds = train_batches(train_data, BATCH_SIZE * NUM_DEVICES)\n",
        "eval_ds = EvalDataset(eval_data, BATCH_SIZE * NUM_DEVICES)\n",
        "\n",
        "print(f\"Train: {len(train_data)} examples | Eval: {len(eval_data)} examples | Batch size: {BATCH_SIZE * NUM_DEVICES}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ToTGxuf9D27"
      },
      "source": [
        "## Provide the training configuration\n",
        "\n",
        "The model expects specific input formats: position indices for rotary embeddings and a 3D causal attention mask `[batch, seq, seq]` combining causal attention with padding. The `gen_model_input` function constructs these from the tokenized batch.\n",
        "\n",
        "`PeftTrainer` orchestrates the training loop with an AdamW optimizer, periodic checkpointing, TensorBoard-compatible metrics logging, and evaluation every `eval_every_n_steps` steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRmCedfn9D27"
      },
      "outputs": [],
      "source": [
        "# Input processing helpers\n",
        "def build_positions(mask):\n",
        "    return jnp.clip(jnp.cumsum(mask, axis=-1) - 1, 0).astype(jnp.int32)\n",
        "\n",
        "def build_causal_mask(mask):\n",
        "    n = mask.shape[-1]\n",
        "    return jnp.tril(jnp.ones((n, n), dtype=jnp.bool_))[None] & mask[:, None, :]\n",
        "\n",
        "def gen_model_input(x):\n",
        "    mask = x[\"input_tokens\"] != tokenizer.pad_token_id\n",
        "    return {\n",
        "        \"input_tokens\": x[\"input_tokens\"],\n",
        "        \"positions\": build_positions(mask),\n",
        "        \"attention_mask\": build_causal_mask(mask),\n",
        "        \"input_mask\": x[\"input_mask\"],\n",
        "    }\n",
        "\n",
        "# Create trainer\n",
        "trainer = peft_trainer.PeftTrainer(\n",
        "    lora_model,\n",
        "    optax.adamw(LEARNING_RATE),\n",
        "    peft_trainer.TrainingConfig(\n",
        "        max_steps=MAX_STEPS,\n",
        "        eval_every_n_steps=25,  # Evaluate every 25 steps\n",
        "        checkpoint_root_directory=f\"{OUTPUT_DIR}/checkpoints\",\n",
        "        metrics_logging_options=metrics_logger.MetricsLoggerOptions(log_dir=f\"{OUTPUT_DIR}/logs\"),\n",
        "    ),\n",
        ").with_gen_model_input_fn(gen_model_input)\n",
        "\n",
        "print(\"Trainer ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e7VliKg9D27"
      },
      "source": [
        "## Run the training\n",
        "\n",
        "This block launches the PEFT training loop. It runs a baseline evaluation first to measure initial loss, then trains for `MAX_STEPS` steps with periodic evaluation. The first step is slower due to XLA JIT compilation, which is cached for subsequent steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5A9rgSC9D27"
      },
      "outputs": [],
      "source": [
        "# Training with progress\n",
        "NUM_EVAL_BATCHES = len(eval_data) // (BATCH_SIZE * NUM_DEVICES)\n",
        "\n",
        "class Progress:\n",
        "    def __init__(self, n): \n",
        "        self.n = n\n",
        "        self.t0 = None\n",
        "        self.eval_count = 0\n",
        "        self.eval_started = False\n",
        "    def on_train_start(self, _): \n",
        "        self.t0 = time.time()\n",
        "        print(\"Training (first step includes JIT)...\")\n",
        "    def on_train_end(self, _): \n",
        "        print(f\"\\nDone in {time.time()-self.t0:.0f}s\")\n",
        "    def on_train_step_start(self, _): \n",
        "        self.eval_started = False\n",
        "    def on_train_step_end(self, _, step, loss, dt):\n",
        "        if step <= 2 or step % 10 == 0:\n",
        "            print(f\"Step {step}/{self.n} | Loss: {float(loss):.4f} | {dt:.1f}s/step\")\n",
        "    def on_eval_step_start(self, _):\n",
        "        if not self.eval_started:\n",
        "            self.eval_count += 1\n",
        "            label = \"Baseline eval\" if self.eval_count == 1 else f\"Eval #{self.eval_count}\"\n",
        "            print(f\"{label}...\", end=\" \", flush=True)\n",
        "            self.eval_started = True\n",
        "    def on_eval_step_end(self, _, eval_loss):\n",
        "        avg_loss = float(eval_loss) / NUM_EVAL_BATCHES\n",
        "        print(f\"loss: {avg_loss:.4f} (avg over {NUM_EVAL_BATCHES} batches)\")\n",
        "\n",
        "trainer.training_hooks = Progress(MAX_STEPS)\n",
        "\n",
        "print(\"Starting (baseline eval + JIT compilation first)...\")\n",
        "with mesh:\n",
        "    trainer.train(train_ds, eval_ds)\n",
        "\n",
        "print(f\"Checkpoints: {OUTPUT_DIR}/checkpoints\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnh044BT9D27"
      },
      "source": [
        "### Visualize training with TensorBoard\n",
        "\n",
        "To monitor training loss and other metrics, launch TensorBoard in a separate terminal:\n",
        "\n",
        "```bash\n",
        "tensorboard --logdir=/workspace/llama3_lora_output/logs --host 0.0.0.0 --port 6006 --load_fast=false\n",
        "```\n",
        "\n",
        "Then open [http://127.0.0.1:6006/](http://127.0.0.1:6006/) in your browser."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpOAsdXX9D27"
      },
      "source": [
        "## Test inference\n",
        "\n",
        "A quick sanity check to verify the fine-tuned model produces coherent output. The code below tokenizes a prompt using the Llama 3.1 chat template, then runs greedy autoregressive generation for up to 10 tokens, stopping early if the model produces an EOS token. This confirms the adapters are applied correctly and the model produces reasonable predictions.\n",
        "\n",
        "Note: this is naive autoregressive generation without KV-caching, so each step recomputes attention over the full sequence. For production use, consider a dedicated serving framework with KV-cache support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF903NJa9D27"
      },
      "outputs": [],
      "source": [
        "# Quick inference test with the fine-tuned LoRA model\n",
        "prompt = \"What is the capital of France?\"\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# Tokenize\n",
        "tokens = jnp.array(tokenizer(text)[\"input_ids\"])[None, :]\n",
        "\n",
        "# Greedy autoregressive generation\n",
        "max_new_tokens = 10\n",
        "generated_ids = []\n",
        "eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "for _ in range(max_new_tokens):\n",
        "    seq_len = tokens.shape[1]\n",
        "    positions = jnp.arange(seq_len)[None, :]\n",
        "    attention_mask = jnp.tril(jnp.ones((seq_len, seq_len), dtype=jnp.bool_))[None, :]\n",
        "\n",
        "    with mesh:\n",
        "        output = lora_model(tokens, positions, None, attention_mask)\n",
        "        logits = output[0] if isinstance(output, tuple) else output\n",
        "\n",
        "    next_token_id = int(jnp.argmax(logits[0, -1]))\n",
        "    generated_ids.append(next_token_id)\n",
        "\n",
        "    if next_token_id == eos_token_id:\n",
        "        break\n",
        "\n",
        "    tokens = jnp.concatenate([tokens, jnp.array([[next_token_id]])], axis=1)\n",
        "\n",
        "# Decode all generated tokens\n",
        "generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Generated ({len(generated_ids)} tokens): '{generated_text}'\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
