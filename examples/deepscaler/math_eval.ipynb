{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWdBNOt5MwNH"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "from datasets import Dataset\n",
        "import grain\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "try:\n",
        "  from GOOGLE_INTERNAL_PACKAGE_PATH.pyglib import gfile\n",
        "  from etils import ecolab\n",
        "\n",
        "  cm = ecolab.adhoc(\n",
        "      source=ecolab.FROM_NOTEBOOK_OR_HEAD,\n",
        "      reload=\"tunix\",\n",
        "      behavior=\"preferred\",\n",
        "      cell_autoreload=True,\n",
        "  )\n",
        "\n",
        "  file_open = gfile.Open\n",
        "\n",
        "  NOTEBOOK_ENV = \"g3\"\n",
        "except Exception:\n",
        "  NOTEBOOK_ENV = \"git\"\n",
        "\n",
        "  import contextlib\n",
        "  cm = contextlib.nullcontext()\n",
        "\n",
        "  file_open = open\n",
        "\n",
        "with cm:\n",
        "  from tunix.models.qwen2 import model as qwen2_lib\n",
        "  from tunix.models.qwen2 import params as qwen2_params_lib\n",
        "  from tunix.generate import sampler as sampler_lib\n",
        "  from tunix.utils import math_utils"
      ]
    },
    {
      "metadata": {
        "id": "uxJgMy49gMym"
      },
      "cell_type": "code",
      "source": [
        "from typing import Any, Dict\n",
        "import jax\n",
        "from tqdm.auto import tqdm\n",
        "import re\n",
        "\n",
        "# Only used for Math500\n",
        "def extract_answer_robust(passage: str) -> str:\n",
        "  if not passage:\n",
        "    return None\n",
        "\n",
        "  # Pattern 1: Look for \\boxed{...} with proper matching braces\n",
        "  # This handles nested braces like \\boxed{\\frac{1}{2}}\n",
        "  stack = []\n",
        "  i = passage.find(\"\\\\boxed\")\n",
        "  if i != -1:\n",
        "    i += 6  # Skip '\\boxed'\n",
        "    # Skip whitespace\n",
        "    while i < len(passage) and passage[i].isspace():\n",
        "      i += 1\n",
        "    if i < len(passage) and passage[i] == \"{\":\n",
        "      i += 1\n",
        "      start = i\n",
        "      brace_count = 1\n",
        "      while i < len(passage) and brace_count > 0:\n",
        "        if passage[i] == \"{\":\n",
        "          brace_count += 1\n",
        "        elif passage[i] == \"}\":\n",
        "          brace_count -= 1\n",
        "        i += 1\n",
        "      if brace_count == 0:\n",
        "        answer = passage[start : i - 1]\n",
        "        return answer.strip()\n",
        "\n",
        "  # Pattern 2: Lenient matching - extract up to common terminators\n",
        "  patterns = [\n",
        "      r\"\\\\boxed\\{([^}]+)\\}\",  # Standard\n",
        "      r\"boxed\\{([^}]+)\\}\",  # Missing backslash\n",
        "      r\"\\\\boxed\\s*\\{(.+?)(?:\\.\\s|\\)\\.|\\.$)\",  # Ends with period\n",
        "      r\"final answer is.*?\\\\boxed\\{([^}]+)\",  # \"final answer is\"\n",
        "      r\"answer is.*?\\\\boxed\\{([^}]+)\",\n",
        "  ]\n",
        "\n",
        "  for pattern in patterns:\n",
        "    matches = re.findall(pattern, passage, re.IGNORECASE | re.DOTALL)\n",
        "    if matches:\n",
        "      answer = matches[-1].strip()\n",
        "      # Clean up\n",
        "      answer = answer.rstrip(\".,;:)\")\n",
        "      # Try to fix common LaTeX issues\n",
        "      if \"\\\\frac\" in answer:\n",
        "        # Count braces - each \\frac needs 2 pairs\n",
        "        open_braces = answer.count(\"{\")\n",
        "        close_braces = answer.count(\"}\")\n",
        "        if open_braces > close_braces:\n",
        "          answer += \"}\" * (open_braces - close_braces)\n",
        "      return answer\n",
        "\n",
        "  # Pattern 3: Super lenient - just find anything after boxed{\n",
        "  super_lenient = r\"boxed\\s*\\{([^\\n]{1,200})\"\n",
        "  matches = re.findall(super_lenient, passage, re.IGNORECASE)\n",
        "  if matches:\n",
        "    answer = matches[-1]\n",
        "    # Find the first reasonable endpoint\n",
        "    for char in [\".\", \")\", \"\\n\", \"The \", \"Thus\", \"Therefore\"]:\n",
        "      if char in answer:\n",
        "        answer = answer[: answer.index(char)]\n",
        "        break\n",
        "    return answer.strip().rstrip(\".,;:)\")\n",
        "\n",
        "  return None"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "_eYdaZj5hB1D"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# only used for AIME-2024\n",
        "THOUGHT_DELIMITER_END = \"</think>\"\n",
        "def evaluate_correctness(response: Any, ground_truths: Any) -> bool:\n",
        "  \"\"\"Evaluate the correctness of a response.\"\"\"\n",
        "  if response is None or response == \"\":\n",
        "    print(f\"{response=} {ground_truths=} IS NOT CORRECT\")\n",
        "    return False\n",
        "  if THOUGHT_DELIMITER_END in response:\n",
        "    response = response.split(THOUGHT_DELIMITER_END)\n",
        "    model_solution = response[1]\n",
        "    print(f\"{model_solution=} after THOUGHT_DELIMITER_END in evaluate_correctness\")\n",
        "  else:\n",
        "    print(f\"{response=} in evaluate_correctness\")\n",
        "    model_solution = response\n",
        "  model_answer = math_utils.extract_answer(model_solution)\n",
        "  if model_answer is None:\n",
        "    print(f\" {model_answer=} {ground_truths=} IS NOT CORRECT\")\n",
        "    return False\n",
        "  if ground_truths is None:\n",
        "    print(f\" {model_answer=} {ground_truths=} IS NOT CORRECT\")\n",
        "    return False\n",
        "  # Convert single answer to list for uniform processing\n",
        "  if isinstance(ground_truths, str | float | int):\n",
        "    ground_truths = [ground_truths]\n",
        "  # Process each ground truth\n",
        "  processed_ground_truths = []\n",
        "  for truth in ground_truths:\n",
        "    truth = str(truth)\n",
        "    if \"\\\\boxed\" in truth:\n",
        "      processed_truth = math_utils.extract_answer(truth)\n",
        "      if processed_truth is not None:\n",
        "        processed_ground_truths.append(processed_truth)\n",
        "    else:\n",
        "      processed_ground_truths.append(truth)\n",
        "  print(f\"{processed_ground_truths=} in evaluate_correctness\")\n",
        "  if not processed_ground_truths:\n",
        "    print(f\" {model_answer=} {ground_truths=} IS NOT CORRECT\")\n",
        "    return False\n",
        "  # Check against all possible correct answers\n",
        "  for ground_truth in processed_ground_truths:\n",
        "    is_correct = math_utils.grade_answer_mathd(\n",
        "        model_answer, ground_truth\n",
        "    ) or math_utils.grade_answer_sympy(model_answer, ground_truth)\n",
        "    if is_correct:\n",
        "      print(f\" {model_answer=} {ground_truth=} IS CORRECT\")\n",
        "      return True\n",
        "  print(f\" {model_answer=} {ground_truths=} IS NOT CORRECT\")\n",
        "  return False"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "dT8fblFYN9ns"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer\n",
        "from pprint import pprint\n",
        "import grain\n",
        "\n",
        "class Qwen25MathEvaluator:\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      model_config,\n",
        "      model_path: str,\n",
        "      dataset: str,\n",
        "      mesh_config=None,\n",
        "      max_prompt_length: int = 1024,  # Increased from 512\n",
        "      max_generation_steps: int = 1024,  # Increased from 512\n",
        "  ):\n",
        "    self.model_config = model_config\n",
        "    self.model_path = model_path\n",
        "    self.dataset = dataset\n",
        "    self.max_prompt_length = max_prompt_length\n",
        "    self.max_generation_steps = max_generation_steps\n",
        "\n",
        "    if mesh_config is None:\n",
        "      # Default: 4-way tensor parallelism\n",
        "      mesh_config = [[1, 4], [\"fsdp\", \"tp\"]]\n",
        "    self.mesh = jax.make_mesh(*mesh_config)\n",
        "    self.tokenizer = None\n",
        "    self.model = None\n",
        "    self.sampler = None\n",
        "\n",
        "    print(f\"Initializing Qwen2.5-Math-1.5B evaluator\")\n",
        "    print(f\"Model path: {model_path}\")\n",
        "    print(f\"Mesh config: {mesh_config}\")\n",
        "    print(f\"Available devices: {jax.devices()}\")\n",
        "\n",
        "  def load_model(self):\n",
        "    print(\"Loading model components...\")\n",
        "\n",
        "    print(\"Loading tokenizer...\")\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "        self.model_path, trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    print(\"Setting up model config...\")\n",
        "\n",
        "\n",
        "    print(\"Loading model from safe tensors...\")\n",
        "    with self.mesh:\n",
        "      self.model = qwen2_params_lib.create_model_from_safe_tensors(\n",
        "          file_dir=self.model_path, config=self.model_config, mesh=self.mesh\n",
        "      )\n",
        "\n",
        "    print(\"Model loaded successfully!\")\n",
        "    print(\"Creating sampler...\")\n",
        "    cache_config = sampler_lib.CacheConfig(\n",
        "        cache_size=self.max_prompt_length + self.max_generation_steps + 100,\n",
        "        num_layers=self.model_config.num_layers,\n",
        "        num_kv_heads=self.model_config.num_kv_heads,\n",
        "        head_dim=self.model_config.head_dim,\n",
        "    )\n",
        "\n",
        "    self.sampler = sampler_lib.Sampler(\n",
        "        transformer=self.model,\n",
        "        tokenizer=self.tokenizer,\n",
        "        cache_config=cache_config,\n",
        "    )\n",
        "\n",
        "    print(\"Sampler created successfully!\")\n",
        "\n",
        "    return {\n",
        "        \"model\": self.model,\n",
        "        \"tokenizer\": self.tokenizer,\n",
        "        \"sampler\": self.sampler,\n",
        "        \"config\": self.model_config,\n",
        "    }\n",
        "\n",
        "  def load_dataset(self, split: str = \"test\") -> grain.MapDataset:\n",
        "    print(f\"Loading {self.dataset} dataset (split: {split})...\")\n",
        "\n",
        "    def preprocess_fn(example, idx):\n",
        "      return {\n",
        "          \"question\": example[\"problem\"],\n",
        "          \"answer\": example[\"answer\"],\n",
        "          \"data_source\": \"math\",\n",
        "          }\n",
        "\n",
        "    with file_open(self.dataset, \"rb\") as test_f:\n",
        "      if self.dataset.endswith(\"jsonl\"):\n",
        "        test_df = pd.read_json(test_f, lines=True)\n",
        "      elif self.dataset.endswith(\"json\"):\n",
        "        test_df = pd.read_json(test_f)\n",
        "      else:\n",
        "        test_df = pd.read_parquet(test_f)\n",
        "\n",
        "    test_ds = Dataset.from_pandas(test_df).map(preprocess_fn, with_indices=True)\n",
        "\n",
        "\n",
        "    print(f\"Loaded {len(test_ds)} examples\")\n",
        "    print(\"Example data:\")\n",
        "    pprint(test_ds[0])\n",
        "\n",
        "    def process_item(item):\n",
        "      question = item[\"question\"]\n",
        "      answer = item[\"answer\"]\n",
        "\n",
        "      if \"aime_2024\" in self.dataset:\n",
        "        instruction = \"Let's think step by step, and put your final answer within \\\\boxed{}.\"\n",
        "        prompt = f\"{question} {instruction}\"\n",
        "      else:\n",
        "        instruction = \"Please reason step by step. Your final answer must appear inside \\\\boxed{...} and nothing else.\"\n",
        "        prompt = f\"{instruction} {question}\"\n",
        "      prompt = self.tokenizer.apply_chat_template(\n",
        "          [{\"role\": \"user\", \"content\": prompt}],\n",
        "          tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "      return {\n",
        "          \"prompt\": prompt,\n",
        "          \"question\": question,\n",
        "          \"answer\": answer,\n",
        "      }\n",
        "\n",
        "    dataset = grain.MapDataset.source(test_ds).map(process_item)\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"DEBUG: First formatted prompt:\")\n",
        "    first_item = dataset[0]\n",
        "    print(first_item[\"prompt\"])\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    return dataset\n",
        "\n",
        "  def generate(\n",
        "      self,\n",
        "      prompts: list[str],\n",
        "      temperature: float = 0.6,\n",
        "      top_k: int = 50,\n",
        "      top_p: float = 0.95,\n",
        "      seed: int = None,\n",
        "  ) -> str:\n",
        "    max_length = max(len(self.tokenizer.encode(p)) for p in prompts)\n",
        "    cache_size = self.max_prompt_length + self.max_generation_steps + 100\n",
        "    safe_gen_length = min(\n",
        "        self.max_generation_steps,\n",
        "        cache_size - max_length - 100,  # 100 token buffer\n",
        "    )\n",
        "    if safe_gen_length < 256:\n",
        "      print(\n",
        "          f\"WARNING: Short generation length ({safe_gen_length} tokens) due to\"\n",
        "          f\" long prompt ({max_length} tokens)\"\n",
        "      )\n",
        "\n",
        "    stop_token_id = self.tokenizer.encode(\"<|im_end|>\")[0]\n",
        "\n",
        "    # Generate\n",
        "    out_data = self.sampler(\n",
        "        input_strings=prompts,\n",
        "        max_generation_steps=safe_gen_length,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        echo=False,\n",
        "        eos_tokens=[stop_token_id],\n",
        "        seed=jax.random.PRNGKey(seed) if seed is not None else None,\n",
        "    )\n",
        "\n",
        "    return out_data.text\n",
        "\n",
        "  def evaluate(\n",
        "      self,\n",
        "      batch_size: int = 8,\n",
        "      num_batches: int = None,\n",
        "      temperature: float = 0.6,\n",
        "      top_k: int = 50,\n",
        "      top_p: float = 0.95,\n",
        "      num_passes: int = 1,\n",
        "      debug_first_n: int = 3,  # NEW: Debug first N examples\n",
        "  ) -> Dict[str, Any]:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Starting Evaluation\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Configuration:\")\n",
        "    print(f\"  Batch size: {batch_size}\")\n",
        "    print(f\"  Num batches: {num_batches or 'all'}\")\n",
        "    print(f\"  Temperature: {temperature}\")\n",
        "    print(f\"  Top-k: {top_k}\")\n",
        "    print(f\"  Top-p: {top_p}\")\n",
        "    print(f\"  Passes per question: {num_passes}\")\n",
        "    print(f\"  Debug first N examples: {debug_first_n}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = self.load_dataset()\n",
        "\n",
        "    # Create batched dataset\n",
        "    if num_batches is not None:\n",
        "      dataset = dataset.batch(batch_size)[:num_batches]\n",
        "    else:\n",
        "      dataset = dataset.batch(batch_size)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    results = []\n",
        "    debug_count = 0\n",
        "\n",
        "    # Evaluate batch by batch\n",
        "    for batch_idx, batch in enumerate(tqdm(dataset, desc=\"Evaluating\")):\n",
        "      prompts = batch[\"prompt\"]\n",
        "\n",
        "      questions = batch[\"question\"]\n",
        "      answers = batch[\"answer\"]\n",
        "\n",
        "      responses_collection = [[] for _ in range(len(prompts))]\n",
        "      for pass_idx in range(num_passes):\n",
        "        batch_response = self.generate(\n",
        "            prompts=prompts,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            top_p=top_p,\n",
        "            seed=pass_idx,\n",
        "        )\n",
        "        for i, r in enumerate(batch_response):\n",
        "          responses_collection[i].append(r)\n",
        "\n",
        "      for prompt, question, answer, responses in zip(\n",
        "          prompts, questions, answers, responses_collection\n",
        "      ):\n",
        "        is_correct = False\n",
        "        extracted_answers = []\n",
        "        answer_correct = []\n",
        "        for response in responses:\n",
        "          # Grade answer using both methods from utils.py\n",
        "          if \"aime_2024\" in self.dataset:\n",
        "            is_correct = evaluate_correctness(response, answer)\n",
        "          else:\n",
        "            model_answer = extract_answer_robust(response)\n",
        "            extracted_answers.append(model_answer)\n",
        "\n",
        "            if model_answer is None:\n",
        "              continue\n",
        "            # Grade answer using both methods from utils.py\n",
        "            is_correct = math_utils.grade_answer_mathd(\n",
        "                model_answer, answer\n",
        "            ) or math_utils.grade_answer_sympy(model_answer, answer)\n",
        "\n",
        "          answer_correct.append(is_correct)\n",
        "\n",
        "          if is_correct:\n",
        "            break\n",
        "\n",
        "        if is_correct:\n",
        "          correct += 1\n",
        "\n",
        "        should_debug = debug_count < debug_first_n\n",
        "\n",
        "        if should_debug:\n",
        "          print(f\"\\n{'='*60}\")\n",
        "          print(f\"DEBUG Example {debug_count + 1}/{debug_first_n}\")\n",
        "          print(f\"Question: {question[:]}\")\n",
        "          print(\"=\" * 60 + \"\\n\")\n",
        "          print(f\"Ground truth: {answer}\")\n",
        "          print(\"=\" * 60 + \"\\n\")\n",
        "          print(f\"Prompt (first 300 chars): {prompt[:]}\")\n",
        "          print(f\"Prompt length: {len(self.tokenizer.encode(prompt))} tokens\")\n",
        "          print(\"=\" * 60 + \"\\n\")\n",
        "          for i, (response, ans, cor) in enumerate(\n",
        "              zip(responses, extracted_answers, answer_correct)\n",
        "          ):\n",
        "            print(f\"Response {i}: {response}\")\n",
        "            print(\"=\" * 120 + \"\\n\")\n",
        "            print(f\"\\nExtracted answer{i}: {ans}\")\n",
        "            print(f\"Is correct: {cor}\")\n",
        "          print(f\"Final result: {'✓ CORRECT' if is_correct else '✗ INCORRECT'}\")\n",
        "          print(\n",
        "              f\"Running accuracy: {correct}/{total+1} =\"\n",
        "              f\" {(correct/(total+1)*100):.2f}%\"\n",
        "          )\n",
        "          debug_count += 1\n",
        "\n",
        "        total += 1\n",
        "\n",
        "        # Store result\n",
        "        results.append({\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"responses\": responses,\n",
        "            \"extracted_answers\": extracted_answers,\n",
        "            \"correct\": is_correct,\n",
        "        })\n",
        "\n",
        "        # Print progress\n",
        "        if total % 10 == 0:\n",
        "          current_acc = (correct / total * 100) if total > 0 else 0\n",
        "          print(f\"\\nProgress: {correct}/{total} = {current_acc:.2f}%\")\n",
        "\n",
        "    # Calculate final metrics\n",
        "    accuracy = (correct / total * 100) if total > 0 else 0\n",
        "\n",
        "    eval_results = {\n",
        "        \"correct\": correct,\n",
        "        \"total\": total,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_k\": top_k,\n",
        "        \"top_p\": top_p,\n",
        "        \"num_passes\": num_passes,\n",
        "        \"detailed_results\": results,\n",
        "    }\n",
        "\n",
        "    return eval_results"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if NOTEBOOK_ENV == \"g3\":\n",
        "    DATA_PATH_PREFIX = \"/GOOGLE_INTERNAL_STOAGE_PATH/gg-d/home/qwix-dev\"\n",
        "    MODEL_PATH_PREFIX = \"/GOOGLE_INTERNAL_STOAGE_PATH/gg-d/home/qwix-dev\"\n",
        "else:\n",
        "    DATA_PATH_PREFIX = \"gs://tunix/data\"\n",
        "    MODEL_PATH_PREFIX = \"gs://tunix/models\"\n",
        "\n",
        "MATH_500_DATA_PATH = os.path.join(DATA_PATH_PREFIX, \"rl/data/MATH-500/test.jsonl\")\n",
        "AIME_2024_DATA_PATH = os.path.join(DATA_PATH_PREFIX, \"rl/data/HuggingFaceH4/aime_2024/train-00000-of-00001.parquet\")\n",
        "MODEL_MAPPING = {\n",
        "    \"qwen2.5-1.5b-it\": (qwen2_lib.ModelConfig.qwen2_5_1_5b() ,os.path.join(MODEL_PATH_PREFIX, \"qwen2_5/torch/1.5b-it\")),\n",
        "    \"DeepSeek-R1-Distill-Qwen-1.5B\": (qwen2_lib.ModelConfig.deepseek_r1_distill_qwen_1_5b(),  os.path.join(MODEL_PATH_PREFIX, \"DeepSeek-R1-Distill-Qwen-1.5B\")),\n",
        "    \"DeepScaleR-1.5B-Preview\": (qwen2_lib.ModelConfig.deepseek_r1_distill_qwen_1_5b(), os.path.join(MODEL_PATH_PREFIX, \"DeepScaleR-1.5B-Preview\")),\n",
        "}\n",
        "\n",
        "mesh_config = [[1, 4], [\"fsdp\", \"tp\"]]  # 4-way tensor parallelism"
      ],
      "metadata": {
        "id": "jHgvS39Jmt49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i1GWrGsIcOws"
      },
      "cell_type": "code",
      "source": [
        "#MATH-500\n",
        "dataset = MATH_500_DATA_PATH\n",
        "model_config, model_path = MODEL_MAPPING[\"qwen2.5-1.5b-it\"]\n",
        "\n",
        "evaluator = Qwen25MathEvaluator(\n",
        "    model_config=model_config,\n",
        "    model_path=model_path,\n",
        "    dataset=dataset,\n",
        "    mesh_config=mesh_config,\n",
        "    max_prompt_length=1024,  # Increased\n",
        "    max_generation_steps=1024,  # Increased\n",
        ")\n",
        "\n",
        "evaluator.load_model()\n",
        "\n",
        "print(\"\\nStarting evaluation...\")\n",
        "results = evaluator.evaluate(\n",
        "    batch_size=8,\n",
        "    # num_batches=3,\n",
        "    temperature=0.6,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_passes=1,\n",
        "    debug_first_n=5,\n",
        ")\n",
        "\n",
        "# Print results\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Evaluation Results\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Model: {model_path}\")\n",
        "print(f\"Dataset: {dataset}\")\n",
        "print(f\"Correct: {results['correct']}/{results['total']}\")\n",
        "print(f\"Accuracy: {results['accuracy']:.2f}%\")\n",
        "print(\"=\" * 60)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# AIME-2024\n",
        "dataset = AIME_2024_DATA_PATH\n",
        "model_config, model_path = MODEL_MAPPING[\"DeepScaleR-1.5B-Preview\"]\n",
        "\n",
        "evaluator = Qwen25MathEvaluator(\n",
        "    model_config=model_config,\n",
        "    model_path=model_path,\n",
        "    dataset=dataset,\n",
        "    mesh_config=mesh_config,\n",
        "    max_prompt_length=2048,  # Increased\n",
        "    max_generation_steps=32768,  # Increased\n",
        ")\n",
        "\n",
        "evaluator.load_model()\n",
        "\n",
        "print(\"\\nStarting evaluation...\")\n",
        "\n",
        "results = evaluator.evaluate(\n",
        "    batch_size=1,\n",
        "    # num_batches=3,\n",
        "    temperature=0.6,\n",
        "    top_k=None,\n",
        "    top_p=0.95,\n",
        "    num_passes=1,\n",
        "    debug_first_n=5,\n",
        ")\n",
        "\n",
        "# Print results\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Evaluation Results\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Model: {model_path}\")\n",
        "print(f\"Dataset: {dataset}\")\n",
        "print(f\"Correct: {results['correct']}/{results['total']}\")\n",
        "print(f\"Accuracy: {results['accuracy']:.2f}%\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "6h7bvKHGkLN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "byautwkgDojq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "last_runtime": {
        "build_target": "//third_party/py/tunix/google/examples:colab_kernel",
        "kind": "private"
      }
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
