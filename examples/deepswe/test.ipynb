{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d75d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [WIP] Reproduction of [DeepSWE](https://www.together.ai/blog/deepswe)\n",
    "# with Multi-turn Agentic framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd579ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datasets as datasets_lib\n",
    "from flax import nnx\n",
    "from jax.sharding import Mesh, NamedSharding, PartitionSpec as P\n",
    "from kubernetes import client, config as k8s_config\n",
    "import numpy as np\n",
    "import optax\n",
    "from orbax import checkpoint as ocp\n",
    "import qwix\n",
    "from transformers import AutoTokenizer\n",
    "from tunix.cli.utils import data as data_lib\n",
    "from tunix.utils import compat\n",
    "\n",
    "Dataset = datasets_lib.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b1321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. Path Setup\n",
    "# ==========================================\n",
    "# Use the absolute path to the ROOT folder\n",
    "pathways_root = os.path.expanduser(\"~/pathways-utils\")\n",
    "r2egym_root = os.path.expanduser(\"~/r2egym\")\n",
    "\n",
    "for root in [pathways_root, r2egym_root]:\n",
    "  if root not in sys.path:\n",
    "    sys.path.insert(0, root)\n",
    "\n",
    "# Verification\n",
    "try:\n",
    "  import pathwaysutils\n",
    "  import r2egym\n",
    "\n",
    "  print(\"✅ pathways-utils, r2egym are successfully mapped.\")\n",
    "except ImportError as e:\n",
    "  print(f\"❌ Still missing a module: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29cc78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. Imports from Custom Modules\n",
    "# ==========================================\n",
    "from tunix.models.qwen3 import params as params_lib\n",
    "from tunix.models.qwen3 import model as model_lib\n",
    "from tunix.sft import utils as sft_utils\n",
    "from tunix.sft import metrics_logger\n",
    "from tunix.rl import rl_cluster as rl_cluster_lib\n",
    "from tunix.rl.rollout import base_rollout\n",
    "from tunix.rl.experimental import agentic_grpo_learner\n",
    "from tunix.rl.agentic.parser.chat_template_parser import parser\n",
    "from tunix.rl.agentic.rewards.reward_types import RewardOutput\n",
    "from system_prompts import (\n",
    "    SWE_SYSTEM_PROMPT,\n",
    "    SWE_SYSTEM_PROMPT_FN_CALL,\n",
    "    SWE_USER_PROMPT,\n",
    "    SWE_USER_PROMPT_FN_CALL,\n",
    "    SWEAGENT_SYSTEM_PROMPT,\n",
    "    SWEAGENT_USER_PROMPT,\n",
    ")\n",
    "\n",
    "# Assumed custom imports based on usage\n",
    "from swe_agent import SWEAgent\n",
    "from swe_env import SWEEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b352d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. Environment Configuration\n",
    "# ==========================================\n",
    "DATASET_CACHE = os.getenv(\n",
    "    \"DATASET_CACHE\", \"/home/sizhi_google_com/dataset_cache\"\n",
    ")\n",
    "os.makedirs(DATASET_CACHE, exist_ok=True)\n",
    "\n",
    "os.environ[\"KUBECONFIG\"] = \"~/.kube/config\"\n",
    "os.environ[\"NODE_SELECTOR_KEY\"] = \"cloud.google.com/gke-nodepool\"\n",
    "os.environ[\"NODE_SELECTOR_VAL\"] = (\n",
    "    \"deepswe-worker-pool\"  # NB: change based on your node pool name\n",
    ")\n",
    "\n",
    "# Kubernetes Setup\n",
    "try:\n",
    "  k8s_config.load_kube_config()\n",
    "  k8s_client = client.CoreV1Api()\n",
    "  # k8s_client.list_namespace(timeout_seconds=5)\n",
    "except Exception as e:\n",
    "  print(f\"Warning: Kubernetes config loading failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. Model & Training Hyperparameters\n",
    "# ==========================================\n",
    "# MODEL_PATH = \"/scratch/models/DeepSeek-R1-Distill-Qwen-1.5B/\"\n",
    "# MODEL_PATH = os.path.expanduser(\"~/models/Qwen3-4B-Instruct-2507/\")\n",
    "MODEL_PATH = os.path.expanduser(\"~/models/Qwen3-1.7B/\")\n",
    "\n",
    "# ====== Data ======\n",
    "TRAIN_FRACTION = 1.0\n",
    "\n",
    "# ====== Reproducibility ======\n",
    "SEED = 42\n",
    "\n",
    "# ====== LoRA ======\n",
    "RANK = 64\n",
    "ALPHA = 64.0\n",
    "TRAIN_WITH_LORA = False\n",
    "\n",
    "# ====== Sharding ======\n",
    "# MESH = [(4, 2), (\"fsdp\", \"tp\")]\n",
    "\n",
    "\n",
    "# ====== GRPO ======\n",
    "# === Generation during GRPO training ===\n",
    "# MAX_PROMPT_LENGTH = 32768\n",
    "MAX_PROMPT_LENGTH = 4096\n",
    "MAX_RESPONSE_LENGTH = 512\n",
    "TEMPERATURE = 0.6\n",
    "TOP_P = 0.95\n",
    "TOP_K = 50\n",
    "NUM_GENERATIONS = 2  # This corresponds to `G` in Algorithm 1\n",
    "\n",
    "# === other GRPO configs ===\n",
    "NUM_ITERATIONS = 1\n",
    "BETA = 0.001\n",
    "EPSILON = 0.2\n",
    "\n",
    "# ====== Training ======\n",
    "BATCH_SIZE = 16\n",
    "MINI_BATCH_SIZE = 16\n",
    "# ROLLOUT_MICRO_BATCH_SIZE = 8\n",
    "# LOGPS_MICRO_BATCH_SIZE = 8\n",
    "NUM_BATCHES = 1\n",
    "NUM_TEST_BATCHES = 50\n",
    "\n",
    "EVAL_EVERY_N_STEPS = 10\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# Number of training steps.\n",
    "MAX_STEPS = 10\n",
    "\n",
    "# Max turns in mult-agent interaction (set to 1 for single-turn)\n",
    "MAX_TURNS = 3\n",
    "\n",
    "# === AdamW, warmup, cosine scheduler ===\n",
    "LEARNING_RATE = 1e-6\n",
    "B1 = 0.9\n",
    "B2 = 0.99\n",
    "WEIGHT_DECAY = 0.1\n",
    "WARMUP_STEPS = int(0.1 * MAX_STEPS)\n",
    "MAX_GRAD_NORM = 0.1\n",
    "\n",
    "# ====== Checkpoint saving ======\n",
    "SAVE_INTERVAL_STEPS = 500\n",
    "MAX_TO_KEEP = 4\n",
    "DO_MEM_PROFILING = False\n",
    "\n",
    "# ====== Inference ======\n",
    "GENERATION_CONFIGS = {\n",
    "    \"greedy\": {\"temperature\": 1e-4, \"top_k\": 1, \"top_p\": 1.0},\n",
    "    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "    \"liberal\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},\n",
    "}\n",
    "\n",
    "# ====== Rollout ======\n",
    "ROLLOUT_ENGINE = \"vanilla\"  # one of \"vanilla\", \"vllm\" or \"sglang_jax\"\n",
    "CKPT_DIR = os.path.join(\"/tmp/cp\", \"deepswe_ckpt/00\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af50d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. JAX Device & Mesh Setup\n",
    "# ==========================================\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "devices = jax.devices()\n",
    "split = int(len(devices) / 2)\n",
    "rollout_devices = np.array(devices[:split]).reshape(2, 2)\n",
    "train_devices = np.array(devices[split:]).reshape(2, 2)\n",
    "\n",
    "rollout_mesh = Mesh(rollout_devices, axis_names=(\"fsdp\", \"tp\"))\n",
    "train_mesh = Mesh(train_devices, axis_names=(\"fsdp\", \"tp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0e3424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 6. Model Initialization\n",
    "# ==========================================\n",
    "print(\"Initializing Model...\")\n",
    "config = model_lib.ModelConfig.qwen3_1p7b()\n",
    "\n",
    "\n",
    "qwen_reference = params_lib.create_model_from_safe_tensors(\n",
    "    MODEL_PATH, config, mesh=train_mesh, dtype=jnp.bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "def get_lora_model(base_model, model_mesh):\n",
    "  lora_provider = qwix.LoraProvider(\n",
    "      module_path=(\n",
    "          \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
    "          \".*attn_vec_einsum\"\n",
    "      ),\n",
    "      rank=RANK,\n",
    "      alpha=ALPHA,\n",
    "  )\n",
    "\n",
    "  model_input = base_model.get_model_input()\n",
    "  lora_model = qwix.apply_lora_to_model(\n",
    "      base_model, lora_provider, **model_input\n",
    "  )\n",
    "\n",
    "  with compat.set_mesh(model_mesh):\n",
    "    state = nnx.state(lora_model)\n",
    "    pspecs = nnx.get_partition_spec(state)\n",
    "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "    nnx.update(lora_model, sharded_state)\n",
    "\n",
    "  return lora_model\n",
    "\n",
    "\n",
    "qwen_actor = get_lora_model(qwen_reference, train_mesh)\n",
    "sft_utils.show_hbm_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecc4e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 7. Tokenizer & Parser\n",
    "# ==========================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH, local_files_only=True, trust_remote_code=True\n",
    ")\n",
    "\n",
    "chat_parser = parser.QwenChatTemplateParser(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e934018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 8. Data Loading\n",
    "# ==========================================\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "print(\"Loading Dataset...\")\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"R2E-Gym/R2E-Gym-V1\", split=\"train\", cache_dir=DATASET_CACHE\n",
    ")\n",
    "\n",
    "\n",
    "def transform(entry):\n",
    "  for k, v in entry.items():\n",
    "    if isinstance(v, list) and k != \"prompts\":\n",
    "      entry[k] = json.dumps(v)\n",
    "\n",
    "  return entry\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    transform,\n",
    "    keep_in_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a44d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 9. Optimizer & Checkpointing\n",
    "# ==========================================\n",
    "checkpointing_options = ocp.CheckpointManagerOptions(\n",
    "    save_interval_steps=SAVE_INTERVAL_STEPS, max_to_keep=MAX_TO_KEEP\n",
    ")\n",
    "metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
    "    log_dir=\"/tmp/tensorboard/grpo\", flush_every_n_steps=2\n",
    ")\n",
    "\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
    "        init_value=0.0,\n",
    "        peak_value=LEARNING_RATE,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        decay_steps=MAX_STEPS,\n",
    "        end_value=0.0,\n",
    "    ),\n",
    "    b1=B1,\n",
    "    b2=B2,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569991bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 10. RL Cluster Setup\n",
    "# ==========================================\n",
    "cluster_config = rl_cluster_lib.ClusterConfig(\n",
    "    role_to_mesh={\n",
    "        rl_cluster_lib.Role.ACTOR: train_mesh,\n",
    "        rl_cluster_lib.Role.REFERENCE: train_mesh,\n",
    "        rl_cluster_lib.Role.ROLLOUT: rollout_mesh,\n",
    "    },\n",
    "    rollout_engine=ROLLOUT_ENGINE,\n",
    "    offload_to_cpu=False,\n",
    "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
    "        actor_optimizer=optimizer,\n",
    "        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
    "        max_steps=MAX_STEPS,\n",
    "        mini_batch_size=MINI_BATCH_SIZE,\n",
    "        train_micro_batch_size=1,\n",
    "        metrics_logging_options=metrics_logging_options,\n",
    "        checkpoint_root_directory=CKPT_DIR,\n",
    "        checkpointing_options=checkpointing_options,\n",
    "    ),\n",
    "    rollout_config=base_rollout.RolloutConfig(\n",
    "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "        kv_cache_size=MAX_PROMPT_LENGTH + MAX_RESPONSE_LENGTH + 256,\n",
    "        temperature=TEMPERATURE,\n",
    "        top_p=TOP_P,\n",
    "        top_k=TOP_K,\n",
    "        eos_tokens=[tokenizer.encode(\"<|im_end|>\")[0]],\n",
    "    ),\n",
    ")\n",
    "\n",
    "rl_cluster = rl_cluster_lib.RLCluster(\n",
    "    actor=qwen_actor,\n",
    "    reference=qwen_reference,\n",
    "    tokenizer=tokenizer,\n",
    "    cluster_config=cluster_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370ffb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 11. Learner & Agent Setup\n",
    "# ==========================================\n",
    "grpo_config = agentic_grpo_learner.GRPOConfig(\n",
    "    num_generations=NUM_GENERATIONS,\n",
    "    num_iterations=NUM_ITERATIONS,\n",
    "    max_response_length=MAX_RESPONSE_LENGTH,\n",
    "    beta=BETA,\n",
    "    epsilon=EPSILON,\n",
    "    system_prompt=SWE_SYSTEM_PROMPT,\n",
    "    max_concurrency=1,\n",
    "    epsilon_high=0.28,\n",
    "    off_policy_steps=0,\n",
    ")\n",
    "\n",
    "\n",
    "# Helper for dummy reward function (placeholder)\n",
    "def dummy_reward_fn(prompts, completions, **kwargs):\n",
    "  return 0\n",
    "\n",
    "\n",
    "agentic_grpo_learner = agentic_grpo_learner.GRPOLearner(\n",
    "    rl_cluster=rl_cluster,\n",
    "    reward_fns=dummy_reward_fn,\n",
    "    agent_class=SWEAgent,\n",
    "    agent_kwargs={},\n",
    "    env_class=SWEEnv,\n",
    "    env_kwargs={\"max_steps\": MAX_TURNS},\n",
    "    algo_config=grpo_config,\n",
    "    prompt_key=\"problem_statement\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 11. process dataset and start training\n",
    "# ==========================================\n",
    "import grain\n",
    "\n",
    "grain_dataset = grain.MapDataset.source(dataset)\n",
    "\n",
    "train_dataset, _ = data_lib.post_init_dataset(\n",
    "    grain_dataset,\n",
    "    tokenizer,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_batches=NUM_BATCHES,\n",
    "    max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "    fraction=TRAIN_FRACTION,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    prompt_key=\"problem_statement\",\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Starting training...\")\n",
    "agentic_grpo_learner.train(train_dataset=train_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
