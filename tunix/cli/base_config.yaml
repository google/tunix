# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

################################## LOAD MODEL ##################################
model_config: &base_model_config
  # Specify model name in style of {model_version}-{model size}, this will use to invoke model config
  model_name: "llama3.1-8b"

  # Chagne model source to your downloading model workflow. Example: "kaggle" | "huggingface" | "gcs" | ""
  model_source: "huggingface"

  # Replace it with your own model checkpoint directory.
  model_id: "meta-llama/Llama-3.1-8B"
  ################################## Checkpoint ##################################
  # for model downloaded from huggingface or kaggle, copy model to local directory.
  model_download_path: "/tmp/models"

  ################################## KAGGLE ##################################
  # If download checkpoint from kaggle, specify your kaggle credentials `KAGGLE_USERNAME` and `KAGGLE_KEY ` in envrionment variable.

  ################################## HF ##################################
  # If download checkpoint from huggingface, specifcy your hf credentials `HF_TOKEN` in environnebt variable. 

  ################################## NNX ##################################
  # Integer seed to create `nnx.Rngs` to mangage the random state.
  rng_seed: 0

  # Display model
  model_display: false

  # Directory used for NNX conversion if downloaded Gemma/Gemma2 from Kaggle source. 
  intermediate_ckpt_dir: "/tmp/intermediate_ckpt/"
  ################################# LoRa #################################
  lora_config:
    module_path: ".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj"
    rank: 16
    alpha: 2.0
    weight_qtype: "nf4"
    tile_size: 256

  ################################## MESH ##################################
  mesh:
    shape: "(2,2)"
    # "('fsdp',)"
    axis_names: "('fsdp','tp')"

actor_model_config:
  <<: *base_model_config

reference_model_config:
 <<: *base_model_config

rollout_model_config:
 <<: *base_model_config

################################## Tokenizer ##################################
tokenizer_config:
  # Usually, `tokenizer_path` should be same as `model_id`, unless you load tokenizer from elsewhere.
  tokenizer_path: "meta-llama/Llama-3.1-8B"

  # Define `T_HF_TOKEN` to access tokenizer from huggingface
  tokenizer_type: "huggingface" # Currently supporting: "sentencepiece", "huggingface"

  add_bos: True

  add_eos: True



################################## DATASET ##################################
# Translation_dataset  "Helsinki-NLP/opus-100" or "mtnt/en-fr" for SFT
# Math_dataset "gsm8k" for grpo 
data_source: ""
data_directory: ""
dataset_name: "Helsinki-NLP/opus-100"
batch_size: 16
num_batches: 3738
max_target_length: 256
num_train_epochs: 1
train_fraction: 1.0
num_test_batches: 100

############################### Optimizer ###############################
# Optimizer config
optimizer_config: &base_optimizer_config
  learning_rate: 1e-5
  opt_type: "adamw"

  # == Optax Optimizer Scheduler ==
  # Belows are function parameters for optax optimizer scheduler. Make sure you specified the corresponding parameters for your schedule_type. 

  schedule_type: "warmup_cosine_decay_schedule"
  value: 1e-5
  peak_value: 3e-5
  init_value: 0.0
  end_value: 0.0
  warmup_ratio: 0.1
  b1: 0.9
  b2: 0.99
  weight_decay: 0.1
  warmup_steps: 5
  decay_steps: 10
  # == Grad clipping ==
  # Grad clipping to prevent large gradients. Found this
  # important to keep KL divergence in check.
  max_grad_norm: 0.1


############################### Training Config ###############################
training_config: &base_training_config
  eval_every_n_steps: 2
  max_steps: 10
  gradient_accumulation_steps: 1
  checkpoint_root_directory: "/tmp/ckpts"
  checkpointing_options:
    max_to_keep: 1
    save_interval_steps: 180
  metrics_logging_options:
    log_dir: "/tmp/logging"
    flush_every_n_steps: 20
  profiler_options:
    log_dir: "/tmp/profiling"
    skip_first_n_steps: 1
    profiler_steps: 9
  data_sharding_axis: ["fsdp"]
  max_inflight_computations: 2

rl_training_config: 
  <<: *base_training_config
  actor_optimizer_config:
    <<: *base_optimizer_config

  critic_optimizer_config: null 
############################### GRPO Config ###############################
grpo_config:
  # The number of times the policy generates multiple responses for a given prompt
  # within a single training step. This corresponds to `G` in Algorithm 1 in the
  # paper. The "group" in GRPO comes from here.
  num_generations: 2

  # The number of iterations per batch (𝜇 in GRPO algo 1).
  num_iterations: 1

  # The coefficient for the KL divergence penalty (𝛽) in the GRPO loss function.
  # Important to keep a high enough value for this, otherwise, the KL divergence
  # can increase unchecked.
  beta: 0.08

  # Epsilon value for clipping (𝜀 in GRPO loss in paper). Similar to PPO, for
  # stable updates.
  epsilon: 0.2


############################### Rollout Config ###############################
rollout_config:
  total_generation_steps: 768

  max_prompt_length: 256 

  # Important to keep a high-ish temperature for varied, diverse responses during
  # training.
  temperature: 0.9

  top_p: 1.0

  top_k: 50


############################### Other RL  Config ###############################

rollout_engine: "vanilla"

offload_to_cpu: false 

 ############################# Reward Fns #############################

verl_compatible: false 
reward_functions: 
 - tunix/cli/reward_fn/gsm8k.py